{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import LongformerTokenizer, LongformerModel, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniforge3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Load the biomedical NER model\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "ner_pipe = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 88 new tokens:\n",
      "\\s_volume\n",
      "\\e_volume\n",
      "\\s_age\n",
      "\\e_age\n",
      "\\s_dosage\n",
      "\\e_dosage\n",
      "\\s_subject\n",
      "\\e_subject\n",
      "\\s_qualitative_concept\n",
      "\\e_qualitative_concept\n",
      "\\s_history\n",
      "\\e_history\n",
      "\\s_disease_disorder\n",
      "\\e_disease_disorder\n",
      "\\s_occupation\n",
      "\\e_occupation\n",
      "\\s_family_history\n",
      "\\e_family_history\n",
      "\\s_activity\n",
      "\\e_activity\n",
      "\\s_outcome\n",
      "\\e_outcome\n",
      "\\s_severity\n",
      "\\e_severity\n",
      "\\s_frequency\n",
      "\\e_frequency\n",
      "\\s_personal_[back](biological_structure\n",
      "\\e_personal_[back](biological_structure\n",
      "\\s_area\n",
      "\\e_area\n",
      "\\s_detailed_description\n",
      "\\e_detailed_description\n",
      "\\s_biological_attribute\n",
      "\\e_biological_attribute\n",
      "\\s_non[biological](detailed_description\n",
      "\\e_non[biological](detailed_description\n",
      "\\s_diagnostic_procedure\n",
      "\\e_diagnostic_procedure\n",
      "\\s_therapeutic_procedure\n",
      "\\e_therapeutic_procedure\n",
      "\\s_height\n",
      "\\e_height\n",
      "\\s_shape\n",
      "\\e_shape\n",
      "\\s_time\n",
      "\\e_time\n",
      "\\s_weight\n",
      "\\e_weight\n",
      "\\s_sign_symptom\n",
      "\\e_sign_symptom\n",
      "\\s_color\n",
      "\\e_color\n",
      "\\s_other_entity\n",
      "\\e_other_entity\n",
      "\\s_medication\n",
      "\\e_medication\n",
      "\\s_administration\n",
      "\\e_administration\n",
      "\\s_date\n",
      "\\e_date\n",
      "\\s_duration\n",
      "\\e_duration\n",
      "\\s_lab_value\n",
      "\\e_lab_value\n",
      "\\s_mass\n",
      "\\e_mass\n",
      "\\s_texture\n",
      "\\e_texture\n",
      "\\s_sex\n",
      "\\e_sex\n",
      "\\s_other_event\n",
      "\\e_other_event\n",
      "\\s_clinical_event\n",
      "\\e_clinical_event\n",
      "\\s_biological_structure\n",
      "\\e_biological_structure\n",
      "\\s_quantitative_concept\n",
      "\\e_quantitative_concept\n",
      "\\s_distance\n",
      "\\e_distance\n",
      "\\s_personal_background\n",
      "\\e_personal_background\n",
      "\\s_coreference\n",
      "\\e_coreference\n",
      "\\s_nonbiological_location\n",
      "\\e_nonbiological_location\n",
      "\\sq\n",
      "\\eq\n"
     ]
    }
   ],
   "source": [
    "ner_tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "ner_pipe = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Extract all unique entity types\n",
    "all_entity_types = ner_model.config.id2label.values()\n",
    "\n",
    "# Process entity types to remove 'B-' and 'I-' prefixes and create a unique set\n",
    "unique_entity_types = set()\n",
    "for entity_type in all_entity_types:\n",
    "    if entity_type.startswith('B-') or entity_type.startswith('I-'):\n",
    "        unique_entity_types.add(entity_type[2:])\n",
    "    elif entity_type != 'O':  # Ignore 'O' (Outside) tag\n",
    "        unique_entity_types.add(entity_type)\n",
    "\n",
    "# Generate start and end tokens for each unique entity type\n",
    "new_tokens = []\n",
    "for entity_type in unique_entity_types:\n",
    "    new_tokens.extend([f'\\\\s_{entity_type.lower()}', f'\\\\e_{entity_type.lower()}'])\n",
    "\n",
    "# Add question start and end tokens\n",
    "new_tokens.extend(['\\\\sq', '\\\\eq'])\n",
    "\n",
    "print(f\"Added {len(new_tokens)} new tokens:\")\n",
    "for token in new_tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the Longformer-based classifier\n",
    "class LongformerClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes):\n",
    "        super(LongformerClassifier, self).__init__()\n",
    "        self.longformer = LongformerModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.05)\n",
    "        self.fc = nn.Linear(self.longformer.config.hidden_size, num_classes)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.longformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token representation\n",
    "        x = self.dropout(pooled_output)\n",
    "        logits = self.fc(x)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "        \n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the dataset\n",
    "class MCQDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prompt = self.data.iloc[idx]['processed_prompt']\n",
    "        label = self.data.iloc[idx]['best_model']\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            prompt,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import LongformerTokenizer, LongformerModel, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.amp import autocast, GradScaler\n",
    "from transformers import LongformerTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def training(experiment_name, lr, train_loader, test_loader, epochs=30):\n",
    "    writer = SummaryWriter(experiment_name)\n",
    "    \n",
    "    # Initialize the tokenizer and model\n",
    "    model_name = \"allenai/longformer-base-4096\"\n",
    "    tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
    "    num_added_tokens = tokenizer.add_tokens(new_tokens)\n",
    "    print(f\"Number of tokens added: {num_added_tokens}\")\n",
    "    \n",
    "    num_classes = 3  # Define num_classes here\n",
    "    model = LongformerClassifier(model_name, num_classes=num_classes)\n",
    "    model.longformer.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(f\"Using device: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Optimizer and scheduler setup\n",
    "    num_training_steps = epochs * len(train_loader)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.2)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=2, num_training_steps=num_training_steps)\n",
    "    \n",
    "    # Initialize the GradScaler for mixed precision training\n",
    "    scaler = GradScaler('cuda')\n",
    "    \n",
    "    # Create directory for saving models\n",
    "    os.makedirs(experiment_name, exist_ok=True)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        all_train_labels = []\n",
    "        all_train_predictions = []\n",
    "        \n",
    "        # Use tqdm for progress bar\n",
    "        train_iterator = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} [Train]\", leave=False)\n",
    "        \n",
    "        for batch in train_iterator:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Use autocast for mixed precision\n",
    "            with autocast('cuda'):\n",
    "                logits, loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            \n",
    "            # Scale the loss and call backward()\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Unscale the gradients, clip them, and update the parameters\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            \n",
    "            # Update the scaler\n",
    "            scaler.update()\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            all_train_labels.extend(labels.cpu().numpy())\n",
    "            all_train_predictions.extend(predictions.cpu().numpy())\n",
    "            \n",
    "            # Update tqdm progress bar\n",
    "            train_iterator.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_accuracy = accuracy_score(all_train_labels, all_train_predictions)\n",
    "        train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(all_train_labels, all_train_predictions, average='weighted', zero_division=0)\n",
    "        \n",
    "        writer.add_scalar('Loss/train', total_train_loss / len(train_loader), epoch)\n",
    "        writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n",
    "        writer.add_scalar('F1/train', train_f1, epoch)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}\")\n",
    "        print(f\"Training - Loss: {total_train_loss / len(train_loader):.4f}, Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}\")\n",
    "        \n",
    "        # Evaluation on test set\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        all_test_labels = []\n",
    "        all_test_predictions = []\n",
    "        \n",
    "        test_iterator = tqdm(test_loader, desc=f\"Epoch {epoch + 1}/{epochs} [Test]\", leave=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_iterator:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                # Use autocast for mixed precision\n",
    "                with autocast('cuda'):\n",
    "                    logits, loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                \n",
    "                total_test_loss += loss.item()\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                all_test_labels.extend(labels.cpu().numpy())\n",
    "                all_test_predictions.extend(predictions.cpu().numpy())\n",
    "                \n",
    "                # Update tqdm progress bar\n",
    "                test_iterator.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        # Calculate test metrics\n",
    "        test_accuracy = accuracy_score(all_test_labels, all_test_predictions)\n",
    "        test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(all_test_labels, all_test_predictions, average='weighted', zero_division=0)\n",
    "        \n",
    "        writer.add_scalar('Loss/test', total_test_loss / len(test_loader), epoch)\n",
    "        writer.add_scalar('Accuracy/test', test_accuracy, epoch)\n",
    "        writer.add_scalar('F1/test', test_f1, epoch)\n",
    "        \n",
    "        print(f\"\\nTest - Loss: {total_test_loss / len(test_loader):.4f}, Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}\")\n",
    "        \n",
    "        # Print per-class metrics for testing\n",
    "        class_names = ['medalpaca', 'biomistral', 'meditron']\n",
    "        test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(\n",
    "            all_test_labels, all_test_predictions, \n",
    "            average=None, zero_division=0, labels=range(num_classes)\n",
    "        )\n",
    "        \n",
    "        for i, class_name in enumerate(class_names):\n",
    "            print(f\"  {class_name} - Precision: {test_precision[i]:.4f}, Recall: {test_recall[i]:.4f}, F1: {test_f1[i]:.4f}\")\n",
    "        \n",
    "        # Save the model after each epoch\n",
    "        torch.save(model.state_dict(), f'/home/ubuntu/nlp/models/model')\n",
    "        print(f\"Model saved: {experiment_name}/model_epoch_{epoch+1}.pt\")\n",
    "\n",
    "    print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess the data\n",
    "df = pd.read_csv('mcq_data_with_custom_ner_tags_cleaned.csv')\n",
    "df =df.sample(4000)\n",
    "# Prepare the data\n",
    "model_name = \"allenai/longformer-base-4096\"\n",
    "tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
    "max_length = 3400  # Longformer can handle up to 4096, adjust based on your needs\n",
    "\n",
    "# Map string labels to integers\n",
    "label_map = {'medalpaca': 0, 'biomistral': 1, 'meditron': 2}\n",
    "df['best_model'] = df['best_model'].map(label_map)\n",
    "\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MCQDataset(train_df, tokenizer, max_length)\n",
    "test_dataset = MCQDataset(test_df, tokenizer, max_length)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 3\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens added: 88\n",
      "Using device: cuda\n",
      "GPU: Tesla T4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]:   0%|          | 0/1067 [00:00<?, ?it/s]Input ids are automatically padded to be a multiple of `config.attention_window`: 512\n",
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training - Loss: 1.0648, Accuracy: 0.4672, Precision: 0.3195, Recall: 0.4672, F1: 0.3199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test - Loss: 1.0881, Accuracy: 0.4363, Precision: 0.1903, Recall: 0.4363, F1: 0.2650\n",
      "  medalpaca - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  biomistral - Precision: 0.4363, Recall: 1.0000, F1: 0.6075\n",
      "  meditron - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Model saved: runs/longformer_biomedical_experiment_lr_1e-05/model_epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training - Loss: 1.0640, Accuracy: 0.4709, Precision: 0.3098, Recall: 0.4709, F1: 0.3047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test - Loss: 1.0631, Accuracy: 0.3600, Precision: 0.3769, Recall: 0.3600, F1: 0.1975\n",
      "  medalpaca - Precision: 0.3581, Recall: 0.9965, F1: 0.5269\n",
      "  biomistral - Precision: 0.5714, Recall: 0.0115, F1: 0.0225\n",
      "  meditron - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Model saved: runs/longformer_biomedical_experiment_lr_1e-05/model_epoch_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training - Loss: 1.0653, Accuracy: 0.4716, Precision: 0.5480, Recall: 0.4716, F1: 0.3095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test - Loss: 1.0720, Accuracy: 0.4363, Precision: 0.1903, Recall: 0.4363, F1: 0.2650\n",
      "  medalpaca - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  biomistral - Precision: 0.4363, Recall: 1.0000, F1: 0.6075\n",
      "  meditron - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Model saved: runs/longformer_biomedical_experiment_lr_1e-05/model_epoch_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Training - Loss: 1.0667, Accuracy: 0.4713, Precision: 0.3303, Recall: 0.4713, F1: 0.3143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test - Loss: 1.0604, Accuracy: 0.4363, Precision: 0.1903, Recall: 0.4363, F1: 0.2650\n",
      "  medalpaca - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  biomistral - Precision: 0.4363, Recall: 1.0000, F1: 0.6075\n",
      "  meditron - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Model saved: runs/longformer_biomedical_experiment_lr_1e-05/model_epoch_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Training - Loss: 1.0647, Accuracy: 0.4728, Precision: 0.3756, Recall: 0.4728, F1: 0.3044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test - Loss: 1.0792, Accuracy: 0.4363, Precision: 0.1903, Recall: 0.4363, F1: 0.2650\n",
      "  medalpaca - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  biomistral - Precision: 0.4363, Recall: 1.0000, F1: 0.6075\n",
      "  meditron - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Model saved: runs/longformer_biomedical_experiment_lr_1e-05/model_epoch_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6\n",
      "Training - Loss: 1.0648, Accuracy: 0.4722, Precision: 0.3753, Recall: 0.4722, F1: 0.3034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test - Loss: 1.0634, Accuracy: 0.4363, Precision: 0.1903, Recall: 0.4363, F1: 0.2650\n",
      "  medalpaca - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  biomistral - Precision: 0.4363, Recall: 1.0000, F1: 0.6075\n",
      "  meditron - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Model saved: runs/longformer_biomedical_experiment_lr_1e-05/model_epoch_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7\n",
      "Training - Loss: 1.0634, Accuracy: 0.4722, Precision: 0.2230, Recall: 0.4722, F1: 0.3029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test - Loss: 1.0617, Accuracy: 0.4363, Precision: 0.1903, Recall: 0.4363, F1: 0.2650\n",
      "  medalpaca - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  biomistral - Precision: 0.4363, Recall: 1.0000, F1: 0.6075\n",
      "  meditron - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Model saved: runs/longformer_biomedical_experiment_lr_1e-05/model_epoch_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8\n",
      "Training - Loss: 1.0635, Accuracy: 0.4722, Precision: 0.2230, Recall: 0.4722, F1: 0.3029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test - Loss: 1.0574, Accuracy: 0.4363, Precision: 0.1903, Recall: 0.4363, F1: 0.2650\n",
      "  medalpaca - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  biomistral - Precision: 0.4363, Recall: 1.0000, F1: 0.6075\n",
      "  meditron - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Model saved: runs/longformer_biomedical_experiment_lr_1e-05/model_epoch_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9\n",
      "Training - Loss: 1.0571, Accuracy: 0.4722, Precision: 0.2230, Recall: 0.4722, F1: 0.3029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test - Loss: 1.0769, Accuracy: 0.4363, Precision: 0.1903, Recall: 0.4363, F1: 0.2650\n",
      "  medalpaca - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  biomistral - Precision: 0.4363, Recall: 1.0000, F1: 0.6075\n",
      "  meditron - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Model saved: runs/longformer_biomedical_experiment_lr_1e-05/model_epoch_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10\n",
      "Training - Loss: 1.0627, Accuracy: 0.4722, Precision: 0.2230, Recall: 0.4722, F1: 0.3029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test - Loss: 1.0647, Accuracy: 0.4363, Precision: 0.1903, Recall: 0.4363, F1: 0.2650\n",
      "  medalpaca - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  biomistral - Precision: 0.4363, Recall: 1.0000, F1: 0.6075\n",
      "  meditron - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Model saved: runs/longformer_biomedical_experiment_lr_1e-05/model_epoch_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11\n",
      "Training - Loss: 1.0604, Accuracy: 0.4722, Precision: 0.2230, Recall: 0.4722, F1: 0.3029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test - Loss: 1.0585, Accuracy: 0.4363, Precision: 0.1903, Recall: 0.4363, F1: 0.2650\n",
      "  medalpaca - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  biomistral - Precision: 0.4363, Recall: 1.0000, F1: 0.6075\n",
      "  meditron - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Model saved: runs/longformer_biomedical_experiment_lr_1e-05/model_epoch_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12\n",
      "Training - Loss: 1.0623, Accuracy: 0.4722, Precision: 0.2230, Recall: 0.4722, F1: 0.3029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test - Loss: 1.0605, Accuracy: 0.4363, Precision: 0.1903, Recall: 0.4363, F1: 0.2650\n",
      "  medalpaca - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  biomistral - Precision: 0.4363, Recall: 1.0000, F1: 0.6075\n",
      "  meditron - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Model saved: runs/longformer_biomedical_experiment_lr_1e-05/model_epoch_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13\n",
      "Training - Loss: 1.0650, Accuracy: 0.4722, Precision: 0.2230, Recall: 0.4722, F1: 0.3029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test - Loss: 1.0634, Accuracy: 0.4363, Precision: 0.1903, Recall: 0.4363, F1: 0.2650\n",
      "  medalpaca - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  biomistral - Precision: 0.4363, Recall: 1.0000, F1: 0.6075\n",
      "  meditron - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Model saved: runs/longformer_biomedical_experiment_lr_1e-05/model_epoch_13.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14\n",
      "Training - Loss: 1.0601, Accuracy: 0.4722, Precision: 0.2230, Recall: 0.4722, F1: 0.3029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test - Loss: 1.0599, Accuracy: 0.4363, Precision: 0.1903, Recall: 0.4363, F1: 0.2650\n",
      "  medalpaca - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  biomistral - Precision: 0.4363, Recall: 1.0000, F1: 0.6075\n",
      "  meditron - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Model saved: runs/longformer_biomedical_experiment_lr_1e-05/model_epoch_14.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15\n",
      "Training - Loss: 1.0624, Accuracy: 0.4722, Precision: 0.2230, Recall: 0.4722, F1: 0.3029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test - Loss: 1.0632, Accuracy: 0.4363, Precision: 0.1903, Recall: 0.4363, F1: 0.2650\n",
      "  medalpaca - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  biomistral - Precision: 0.4363, Recall: 1.0000, F1: 0.6075\n",
      "  meditron - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Model saved: runs/longformer_biomedical_experiment_lr_1e-05/model_epoch_15.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 [Train]: 100%|█████████▉| 1066/1067 [36:37<00:02,  2.06s/it, loss=1.2821]"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "learning_rates = [1e-5]  # You can adjust this list of learning rates\n",
    "for lr in learning_rates:\n",
    "    training(f'runs/longformer_biomedical_experiment_lr_{str(lr)}', lr, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LongformerClassifier:\n\tsize mismatch for longformer.embeddings.word_embeddings.weight: copying a param with shape torch.Size([50353, 768]) from checkpoint, the shape in current model is torch.Size([50265, 768]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m data_csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmcq_data_with_custom_ner_tags_cleaned.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Your input CSV\u001b[39;00m\n\u001b[1;32m     59\u001b[0m output_csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompts_with_predictions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Name of the output CSV\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[43mgenerate_predictions_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_csv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_csv_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m, in \u001b[0;36mgenerate_predictions_csv\u001b[0;34m(model_path, data_csv_path, output_csv_path, batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m LongformerClassifier(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallenai/longformer-base-4096\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LongformerClassifier:\n\tsize mismatch for longformer.embeddings.word_embeddings.weight: copying a param with shape torch.Size([50353, 768]) from checkpoint, the shape in current model is torch.Size([50265, 768])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import LongformerTokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have already defined LongformerClassifier and MCQDataset classes\n",
    "\n",
    "def generate_predictions_csv(model_path, data_csv_path, output_csv_path, batch_size=8):\n",
    "    # Load the trained model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LongformerClassifier(\"allenai/longformer-base-4096\", num_classes=3)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load and prepare the data\n",
    "    df = pd.read_csv(data_csv_path)\n",
    "    tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "    max_length = 3400\n",
    "\n",
    "    # Add new tokens if they were used during training\n",
    "    tokenizer.add_tokens(new_tokens)  # Assuming new_tokens is defined in the global scope\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = MCQDataset(df, tokenizer, max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Make predictions\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Generating predictions\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits, _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    # Map predictions back to model names\n",
    "    label_map = {0: 'medalpaca', 1: 'biomistral', 2: 'meditron'}\n",
    "    predicted_models = [label_map[pred] for pred in all_predictions]\n",
    "\n",
    "    # Create a new dataframe with prompts and predictions\n",
    "    output_df = pd.DataFrame({\n",
    "        'prompt': df['processed_prompt'],\n",
    "        'predicted_best_model': predicted_models\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")\n",
    "\n",
    "# Usage\n",
    "model_path = r'/home/ubuntu/nlp/models/model'  # Replace with your model's path\n",
    "data_csv_path = 'mcq_data_with_custom_ner_tags_cleaned.csv'  # Your input CSV\n",
    "output_csv_path = 'prompts_with_predictions.csv'  # Name of the output CSV\n",
    "\n",
    "generate_predictions_csv(model_path, data_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens added: 20\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LongformerClassifier:\n\tsize mismatch for longformer.embeddings.word_embeddings.weight: copying a param with shape torch.Size([50353, 768]) from checkpoint, the shape in current model is torch.Size([50285, 768]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m data_csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmcq_data_with_custom_ner_tags_cleaned.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Your input CSV\u001b[39;00m\n\u001b[1;32m     68\u001b[0m output_csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompts_with_predictions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Name of the output CSV\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[43mgenerate_predictions_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_csv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_csv_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 26\u001b[0m, in \u001b[0;36mgenerate_predictions_csv\u001b[0;34m(model_path, data_csv_path, output_csv_path, batch_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Resize token embeddings before loading state dict\u001b[39;00m\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mlongformer\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n\u001b[0;32m---> 26\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LongformerClassifier:\n\tsize mismatch for longformer.embeddings.word_embeddings.weight: copying a param with shape torch.Size([50353, 768]) from checkpoint, the shape in current model is torch.Size([50285, 768])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import LongformerTokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have already defined LongformerClassifier and MCQDataset classes\n",
    "\n",
    "def generate_predictions_csv(model_path, data_csv_path, output_csv_path, batch_size=8):\n",
    "    # Initialize tokenizer and add custom tokens\n",
    "    tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "    \n",
    "    # Define new_tokens (make sure this matches what you used during training)\n",
    "    new_tokens = ['\\sq', '\\eq'] + [f'\\\\s_{entity.lower()}' for entity in ['GENE', 'DISEASE', 'CHEMICAL', 'SPECIES', 'CELL_LINE', 'DNA', 'RNA', 'CELL_TYPE', 'PROTEIN']] + [f'\\\\e_{entity.lower()}' for entity in ['GENE', 'DISEASE', 'CHEMICAL', 'SPECIES', 'CELL_LINE', 'DNA', 'RNA', 'CELL_TYPE', 'PROTEIN']]\n",
    "    \n",
    "    num_added_tokens = tokenizer.add_tokens(new_tokens)\n",
    "    print(f\"Number of tokens added: {num_added_tokens}\")\n",
    "\n",
    "    # Load the trained model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LongformerClassifier(\"allenai/longformer-base-4096\", num_classes=3)\n",
    "    \n",
    "    # Resize token embeddings before loading state dict\n",
    "    model.longformer.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load and prepare the data\n",
    "    df = pd.read_csv(data_csv_path)\n",
    "    max_length = 3400\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = MCQDataset(df, tokenizer, max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Make predictions\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Generating predictions\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits, _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    # Map predictions back to model names\n",
    "    label_map = {0: 'medalpaca', 1: 'biomistral', 2: 'meditron'}\n",
    "    predicted_models = [label_map[pred] for pred in all_predictions]\n",
    "\n",
    "    # Create a new dataframe with prompts and predictions\n",
    "    output_df = pd.DataFrame({\n",
    "        'prompt': df['processed_prompt'],\n",
    "        'predicted_best_model': predicted_models\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")\n",
    "\n",
    "# Usage\n",
    "model_path = r'/home/ubuntu/nlp/model.pth'  # Replace with your model's path\n",
    "data_csv_path = 'mcq_data_with_custom_ner_tags_cleaned.csv'  # Your input CSV\n",
    "output_csv_path = 'prompts_with_predictions.csv'  # Name of the output CSV\n",
    "\n",
    "generate_predictions_csv(model_path, data_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens added: 20\n",
      "Resizing model embeddings from 50285 to 50353\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LongformerClassifier:\n\tMissing key(s) in state_dict: \"classifier.weight\", \"classifier.bias\". \n\tUnexpected key(s) in state_dict: \"fc.weight\", \"fc.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 102\u001b[0m\n\u001b[1;32m     99\u001b[0m data_csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmcq_data_with_custom_ner_tags_cleaned.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Your input CSV\u001b[39;00m\n\u001b[1;32m    100\u001b[0m output_csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompts_with_predictions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Name of the output CSV\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m \u001b[43mgenerate_predictions_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_csv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_csv_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 59\u001b[0m, in \u001b[0;36mgenerate_predictions_csv\u001b[0;34m(model_path, data_csv_path, output_csv_path, batch_size)\u001b[0m\n\u001b[1;32m     56\u001b[0m     resize_embeddings(model, saved_vocab_size)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Load the state dict\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     61\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LongformerClassifier:\n\tMissing key(s) in state_dict: \"classifier.weight\", \"classifier.bias\". \n\tUnexpected key(s) in state_dict: \"fc.weight\", \"fc.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import LongformerTokenizer, LongformerModel\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have already defined MCQDataset class\n",
    "\n",
    "class LongformerClassifier(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_classes):\n",
    "        super(LongformerClassifier, self).__init__()\n",
    "        self.longformer = LongformerModel.from_pretrained(model_name)\n",
    "        self.classifier = torch.nn.Linear(self.longformer.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.longformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "def resize_embeddings(model, new_num_tokens):\n",
    "    old_embeddings = model.longformer.embeddings.word_embeddings\n",
    "    new_embeddings = torch.nn.Embedding(new_num_tokens, old_embeddings.embedding_dim)\n",
    "    new_embeddings.to(old_embeddings.weight.device)\n",
    "\n",
    "    # Copy the original embeddings\n",
    "    num_tokens_to_copy = min(old_embeddings.num_embeddings, new_num_tokens)\n",
    "    new_embeddings.weight.data[:num_tokens_to_copy, :] = old_embeddings.weight.data[:num_tokens_to_copy, :]\n",
    "\n",
    "    model.longformer.embeddings.word_embeddings = new_embeddings\n",
    "    model.longformer.config.vocab_size = new_num_tokens\n",
    "\n",
    "def generate_predictions_csv(model_path, data_csv_path, output_csv_path, batch_size=8):\n",
    "    # Initialize tokenizer and add custom tokens\n",
    "    tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "    \n",
    "    # Define new_tokens (make sure this matches what you used during training)\n",
    "    new_tokens = ['\\sq', '\\eq'] + [f'\\\\s_{entity.lower()}' for entity in ['GENE', 'DISEASE', 'CHEMICAL', 'SPECIES', 'CELL_LINE', 'DNA', 'RNA', 'CELL_TYPE', 'PROTEIN']] + [f'\\\\e_{entity.lower()}' for entity in ['GENE', 'DISEASE', 'CHEMICAL', 'SPECIES', 'CELL_LINE', 'DNA', 'RNA', 'CELL_TYPE', 'PROTEIN']]\n",
    "    \n",
    "    num_added_tokens = tokenizer.add_tokens(new_tokens)\n",
    "    print(f\"Number of tokens added: {num_added_tokens}\")\n",
    "\n",
    "    # Load the trained model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LongformerClassifier(\"allenai/longformer-base-4096\", num_classes=3)\n",
    "    \n",
    "    # Load state dict\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Get the vocabulary size from the saved model\n",
    "    saved_vocab_size = state_dict['longformer.embeddings.word_embeddings.weight'].size(0)\n",
    "    \n",
    "    # Resize model embeddings if necessary\n",
    "    if saved_vocab_size != len(tokenizer):\n",
    "        print(f\"Resizing model embeddings from {len(tokenizer)} to {saved_vocab_size}\")\n",
    "        resize_embeddings(model, saved_vocab_size)\n",
    "    \n",
    "    # Load the state dict\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load and prepare the data\n",
    "    df = pd.read_csv(data_csv_path)\n",
    "    max_length = 3400\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = MCQDataset(df, tokenizer, max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Make predictions\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Generating predictions\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    # Map predictions back to model names\n",
    "    label_map = {0: 'medalpaca', 1: 'biomistral', 2: 'meditron'}\n",
    "    predicted_models = [label_map[pred] for pred in all_predictions]\n",
    "\n",
    "    # Create a new dataframe with prompts and predictions\n",
    "    output_df = pd.DataFrame({\n",
    "        'prompt': df['processed_prompt'],\n",
    "        'predicted_best_model': predicted_models\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")\n",
    "\n",
    "# Usage\n",
    "model_path = r'/home/ubuntu/nlp/model.pth'  # Replace with your model's path\n",
    "data_csv_path = 'mcq_data_with_custom_ner_tags_cleaned.csv'  # Your input CSV\n",
    "output_csv_path = 'prompts_with_predictions.csv'  # Name of the output CSV\n",
    "\n",
    "generate_predictions_csv(model_path, data_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens added: 20\n",
      "Tokenizer vocabulary size after adding tokens: 50285\n",
      "Saved model vocabulary size: 50353\n",
      "Resizing model embeddings from 50285 to 50353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 1255/1255 [52:17<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to prompts_with_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import LongformerTokenizer, LongformerModel\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MCQDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.label_map = {'medalpaca': 0, 'biomistral': 1, 'meditron': 2}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prompt = self.data.iloc[idx]['processed_prompt']\n",
    "        label = self.data.iloc[idx]['best_model']\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            prompt,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(self.label_map[label], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# [The LongformerClassifier class remains the same as in the previous response]\n",
    "\n",
    "def generate_predictions_csv(model_path, data_csv_path, output_csv_path, batch_size=8):\n",
    "    # Initialize tokenizer and add custom tokens\n",
    "    tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "    \n",
    "    # Define new_tokens (make sure this matches exactly what you used during training)\n",
    "    new_tokens = ['\\sq', '\\eq'] + [f'\\\\s_{entity.lower()}' for entity in ['GENE', 'DISEASE', 'CHEMICAL', 'SPECIES', 'CELL_LINE', 'DNA', 'RNA', 'CELL_TYPE', 'PROTEIN']] + [f'\\\\e_{entity.lower()}' for entity in ['GENE', 'DISEASE', 'CHEMICAL', 'SPECIES', 'CELL_LINE', 'DNA', 'RNA', 'CELL_TYPE', 'PROTEIN']]\n",
    "    \n",
    "    num_added_tokens = tokenizer.add_tokens(new_tokens)\n",
    "    print(f\"Number of tokens added: {num_added_tokens}\")\n",
    "    print(f\"Tokenizer vocabulary size after adding tokens: {len(tokenizer)}\")\n",
    "\n",
    "    # Load the trained model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LongformerClassifier(\"allenai/longformer-base-4096\", num_classes=3)\n",
    "    \n",
    "    # Load state dict\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Get the vocabulary size from the saved model\n",
    "    saved_vocab_size = state_dict['longformer.embeddings.word_embeddings.weight'].size(0)\n",
    "    print(f\"Saved model vocabulary size: {saved_vocab_size}\")\n",
    "    \n",
    "    # Resize model embeddings if necessary\n",
    "    if saved_vocab_size != len(tokenizer):\n",
    "        print(f\"Resizing model embeddings from {len(tokenizer)} to {saved_vocab_size}\")\n",
    "        model.longformer.resize_token_embeddings(saved_vocab_size)\n",
    "    \n",
    "    # Load the state dict\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load and prepare the data\n",
    "    df = pd.read_csv(data_csv_path)\n",
    "    max_length = 3400\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = MCQDataset(df, tokenizer, max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Make predictions\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Generating predictions\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    # Map predictions back to model names\n",
    "    label_map = {0: 'medalpaca', 1: 'biomistral', 2: 'meditron'}\n",
    "    predicted_models = [label_map[pred] for pred in all_predictions]\n",
    "\n",
    "    # Create a new dataframe with prompts and predictions\n",
    "    output_df = pd.DataFrame({\n",
    "        'prompt': df['processed_prompt'],\n",
    "        'predicted_best_model': predicted_models\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")\n",
    "\n",
    "# Usage\n",
    "model_path = r'/home/ubuntu/nlp/model.pth'  # Replace with your model's path\n",
    "data_csv_path = 'mcq_data_with_custom_ner_tags_cleaned.csv'  # Your input CSV\n",
    "output_csv_path = 'prompts_with_predictions.csv'  # Name of the output CSV\n",
    "\n",
    "generate_predictions_csv(model_path, data_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
