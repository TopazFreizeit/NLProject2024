{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['summary', 'prompt', 'correct_answer', 'biomistral', 'meditron',\n",
      "       'medalpaca', 'models_are_correct', 'ids_are_correct'],\n",
      "      dtype='object')\n",
      "10034\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>prompt</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>biomistral</th>\n",
       "      <th>meditron</th>\n",
       "      <th>medalpaca</th>\n",
       "      <th>models_are_correct</th>\n",
       "      <th>ids_are_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The key medical concepts being tested in this ...</td>\n",
       "      <td>The following are multiple choice questions (w...</td>\n",
       "      <td>B</td>\n",
       "      <td>{'A': 0.04511607810854912, 'B': 0.594287753105...</td>\n",
       "      <td>{'A': 0.16622786223888397, 'B': 0.194340050220...</td>\n",
       "      <td>{'A': 0.253202885389328, 'B': 0.32007843255996...</td>\n",
       "      <td>['biomistral', 'medalpaca']</td>\n",
       "      <td>[0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The medical concepts being tested in this ques...</td>\n",
       "      <td>The following are multiple choice questions (w...</td>\n",
       "      <td>C</td>\n",
       "      <td>{'A': 0.20254969596862793, 'B': 0.020370958372...</td>\n",
       "      <td>{'A': 0.173625186085701, 'B': 0.19369290769100...</td>\n",
       "      <td>{'A': 0.2113712579011917, 'B': 0.1725161820650...</td>\n",
       "      <td>['biomistral', 'meditron', 'medalpaca']</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The medical concepts being tested in this ques...</td>\n",
       "      <td>The following are multiple choice questions (w...</td>\n",
       "      <td>C</td>\n",
       "      <td>{'A': 0.2921173572540283, 'B': 0.2701644003391...</td>\n",
       "      <td>{'A': 0.26627469062805176, 'B': 0.204159900546...</td>\n",
       "      <td>{'A': 0.18825779855251312, 'B': 0.325280308723...</td>\n",
       "      <td>['meditron']</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The key medical concepts being tested in this ...</td>\n",
       "      <td>The following are multiple choice questions (w...</td>\n",
       "      <td>A</td>\n",
       "      <td>{'A': 0.22638443112373352, 'B': 0.016919802874...</td>\n",
       "      <td>{'A': 0.1531776636838913, 'B': 0.1708820462226...</td>\n",
       "      <td>{'A': 0.21118728816509247, 'B': 0.198392108082...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The key medical concepts being tested in this ...</td>\n",
       "      <td>The following are multiple choice questions (w...</td>\n",
       "      <td>B</td>\n",
       "      <td>{'A': 0.11145520955324173, 'B': 0.427251458168...</td>\n",
       "      <td>{'A': 0.14759910106658936, 'B': 0.186582759022...</td>\n",
       "      <td>{'A': 0.3045741617679596, 'B': 0.2335251122713...</td>\n",
       "      <td>['biomistral']</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             summary  \\\n",
       "0  The key medical concepts being tested in this ...   \n",
       "1  The medical concepts being tested in this ques...   \n",
       "2  The medical concepts being tested in this ques...   \n",
       "3  The key medical concepts being tested in this ...   \n",
       "4  The key medical concepts being tested in this ...   \n",
       "\n",
       "                                              prompt correct_answer  \\\n",
       "0  The following are multiple choice questions (w...              B   \n",
       "1  The following are multiple choice questions (w...              C   \n",
       "2  The following are multiple choice questions (w...              C   \n",
       "3  The following are multiple choice questions (w...              A   \n",
       "4  The following are multiple choice questions (w...              B   \n",
       "\n",
       "                                          biomistral  \\\n",
       "0  {'A': 0.04511607810854912, 'B': 0.594287753105...   \n",
       "1  {'A': 0.20254969596862793, 'B': 0.020370958372...   \n",
       "2  {'A': 0.2921173572540283, 'B': 0.2701644003391...   \n",
       "3  {'A': 0.22638443112373352, 'B': 0.016919802874...   \n",
       "4  {'A': 0.11145520955324173, 'B': 0.427251458168...   \n",
       "\n",
       "                                            meditron  \\\n",
       "0  {'A': 0.16622786223888397, 'B': 0.194340050220...   \n",
       "1  {'A': 0.173625186085701, 'B': 0.19369290769100...   \n",
       "2  {'A': 0.26627469062805176, 'B': 0.204159900546...   \n",
       "3  {'A': 0.1531776636838913, 'B': 0.1708820462226...   \n",
       "4  {'A': 0.14759910106658936, 'B': 0.186582759022...   \n",
       "\n",
       "                                           medalpaca  \\\n",
       "0  {'A': 0.253202885389328, 'B': 0.32007843255996...   \n",
       "1  {'A': 0.2113712579011917, 'B': 0.1725161820650...   \n",
       "2  {'A': 0.18825779855251312, 'B': 0.325280308723...   \n",
       "3  {'A': 0.21118728816509247, 'B': 0.198392108082...   \n",
       "4  {'A': 0.3045741617679596, 'B': 0.2335251122713...   \n",
       "\n",
       "                        models_are_correct ids_are_correct  \n",
       "0              ['biomistral', 'medalpaca']          [0, 2]  \n",
       "1  ['biomistral', 'meditron', 'medalpaca']       [0, 1, 2]  \n",
       "2                             ['meditron']             [1]  \n",
       "3                                       []              []  \n",
       "4                           ['biomistral']             [0]  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('summary_models_are_correct.csv')\n",
    "print(df.columns)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniforge3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing the rows with empty list of 'ids_are_correct' and 'models_are_correct' current length of the dataset is:  10034\n",
      "After removing the rows with empty list of 'ids_are_correct' and 'models_are_correct' current length of the dataset is:  7322\n",
      "Dataset shape: (7322, 8)\n",
      "Columns: Index(['summary', 'prompt', 'correct_answer', 'biomistral', 'meditron',\n",
      "       'medalpaca', 'models_are_correct', 'ids_are_correct'],\n",
      "      dtype='object')\n",
      "\n",
      "Label distribution:\n",
      "models_are_correct\n",
      "1    3626\n",
      "2    2608\n",
      "3    1088\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of unique labels: 3\n",
      "Label frequencies:\n",
      "biomistral: 4426\n",
      "medalpaca: 4307\n",
      "meditron: 3373\n",
      "\n",
      "Loading tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1835' max='1835' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1835/1835 40:27, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.950800</td>\n",
       "      <td>0.942301</td>\n",
       "      <td>0.558926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.940900</td>\n",
       "      <td>0.944792</td>\n",
       "      <td>0.558926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.956200</td>\n",
       "      <td>0.942166</td>\n",
       "      <td>0.558926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.973500</td>\n",
       "      <td>0.943340</td>\n",
       "      <td>0.558926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.868600</td>\n",
       "      <td>1.000438</td>\n",
       "      <td>0.636613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating the model and calculating metrics...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set results:\n",
      "Accuracy: 0.1618\n",
      "Precision: 0.5685\n",
      "Recall: 0.7387\n",
      "F1 Score: 0.6366\n",
      "\n",
      "All computed metrics from evaluate():\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_f1: 0.6366\n",
      "eval_loss: 1.0004\n",
      "eval_runtime: 23.6902\n",
      "eval_samples_per_second: 61.8400\n",
      "eval_steps_per_second: 3.8830\n",
      "epoch: 5.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data loading and preprocessing\n",
    "df = pd.read_csv('summary_models_are_correct.csv')\n",
    "print(\"Removing the rows with empty list of 'ids_are_correct' and 'models_are_correct' current length of the dataset is: \", len(df))\n",
    "df = df[df['models_are_correct'] != '[]']\n",
    "print(\"After removing the rows with empty list of 'ids_are_correct' and 'models_are_correct' current length of the dataset is: \", len(df))\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns}\")\n",
    "\n",
    "def safe_eval(x):\n",
    "    try:\n",
    "        return ast.literal_eval(x) if isinstance(x, str) else x\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "df['models_are_correct'] = df['models_are_correct'].apply(safe_eval)\n",
    "\n",
    "# Analyze label distribution\n",
    "label_counts = df['models_are_correct'].apply(len).value_counts().sort_index()\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(label_counts)\n",
    "\n",
    "# Create binary labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "binary_labels = mlb.fit_transform(df['models_are_correct'])\n",
    "\n",
    "print(f\"\\nNumber of unique labels: {len(mlb.classes_)}\")\n",
    "print(\"Label frequencies:\")\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    print(f\"{label}: {binary_labels[:, i].sum()}\")\n",
    "\n",
    "# Preprocess text data\n",
    "def preprocess_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "df['preprocessed_summary'] = df['summary'].apply(preprocess_text)\n",
    "\n",
    "# Split the data\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['preprocessed_summary'].tolist(), binary_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Compute class weights based on the number of classes in your labels\n",
    "num_classes = len(mlb.classes_)\n",
    "class_weights = compute_class_weight('balanced', classes=np.arange(num_classes), y=np.argmax(train_labels, axis=1))\n",
    "class_weight_dict = dict(zip(range(num_classes), class_weights))\n",
    "\n",
    "# Tokenizer and model\n",
    "print(\"\\nLoading tokenizer and model...\")\n",
    "model_name = 'microsoft/deberta-v3-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=len(mlb.classes_),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "# Custom dataset\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length=512)\n",
    "test_dataset = TextClassificationDataset(test_texts, test_labels, tokenizer, max_length=512)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = (logits > 0).astype(int)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return {\n",
    "        'eval_f1': f1,\n",
    "    }\n",
    "    \n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    learning_rate=2e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,  # Set to True for F1 score\n",
    ")\n",
    "\n",
    "class WeightedBCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, weight=None):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Ensure input and target have the same number of classes\n",
    "        if input.size(1) != target.size(1):\n",
    "            target = F.pad(target, (0, input.size(1) - target.size(1)), \"constant\", 0)\n",
    "        \n",
    "        loss = F.binary_cross_entropy_with_logits(input, target, reduction='none')\n",
    "        if self.weight is not None:\n",
    "            loss = loss * self.weight.unsqueeze(0)\n",
    "        return loss.mean()\n",
    "\n",
    "# Replace your existing CustomTrainer class with this updated version\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Ensure class_weights tensor has the same number of classes as logits\n",
    "        if logits.size(1) != len(class_weights):\n",
    "            class_weights_tensor = F.pad(torch.tensor(class_weights), (0, logits.size(1) - len(class_weights)), \"constant\", 1.0)\n",
    "        else:\n",
    "            class_weights_tensor = torch.tensor(class_weights)\n",
    "        \n",
    "        loss_fct = WeightedBCEWithLogitsLoss(weight=class_weights_tensor.to(model.device))\n",
    "        loss = loss_fct(logits, labels.float())\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating the model and calculating metrics...\n",
      "\n",
      "Test set results:\n",
      "Accuracy: 0.1618\n",
      "Precision: 0.5685\n",
      "Recall: 0.7387\n",
      "F1 Score: 0.6366\n",
      "\n",
      "All computed metrics from evaluate():\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='92' max='92' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [92/92 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_f1: 0.6366\n",
      "eval_loss: 1.0004\n",
      "eval_runtime: 23.9309\n",
      "eval_samples_per_second: 61.2180\n",
      "eval_steps_per_second: 3.8440\n",
      "epoch: 5.0000\n",
      "\n",
      "Training history:\n",
      "[{'loss': 0.9628, 'grad_norm': 1.049638271331787, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.027247956403269755, 'step': 10}, {'loss': 0.9652, 'grad_norm': 1.549876093864441, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.05449591280653951, 'step': 20}, {'loss': 0.9642, 'grad_norm': 3.034794330596924, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.08174386920980926, 'step': 30}, {'loss': 0.9614, 'grad_norm': 2.1240005493164062, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.10899182561307902, 'step': 40}, {'loss': 0.9633, 'grad_norm': 2.178948163986206, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.1362397820163488, 'step': 50}, {'loss': 0.9623, 'grad_norm': 1.0875111818313599, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.16348773841961853, 'step': 60}, {'loss': 0.9639, 'grad_norm': 2.6517090797424316, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.1907356948228883, 'step': 70}, {'loss': 0.9505, 'grad_norm': 1.2780969142913818, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.21798365122615804, 'step': 80}, {'loss': 0.957, 'grad_norm': 1.1750397682189941, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.2452316076294278, 'step': 90}, {'loss': 0.9547, 'grad_norm': 1.9740135669708252, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.2724795640326976, 'step': 100}, {'loss': 0.9582, 'grad_norm': 0.8445051312446594, 'learning_rate': 4.4e-06, 'epoch': 0.2997275204359673, 'step': 110}, {'loss': 0.9525, 'grad_norm': 1.1661909818649292, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.32697547683923706, 'step': 120}, {'loss': 0.9458, 'grad_norm': 1.0427428483963013, 'learning_rate': 5.2e-06, 'epoch': 0.3542234332425068, 'step': 130}, {'loss': 0.9493, 'grad_norm': 1.5810948610305786, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.3814713896457766, 'step': 140}, {'loss': 0.9556, 'grad_norm': 2.5598182678222656, 'learning_rate': 6e-06, 'epoch': 0.4087193460490463, 'step': 150}, {'loss': 0.9509, 'grad_norm': 2.0420122146606445, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.4359673024523161, 'step': 160}, {'loss': 0.9628, 'grad_norm': 1.837005376815796, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.46321525885558584, 'step': 170}, {'loss': 0.9438, 'grad_norm': 1.7472878694534302, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.4904632152588556, 'step': 180}, {'loss': 0.9598, 'grad_norm': 1.1931134462356567, 'learning_rate': 7.600000000000001e-06, 'epoch': 0.5177111716621253, 'step': 190}, {'loss': 0.948, 'grad_norm': 1.7359261512756348, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.5449591280653951, 'step': 200}, {'loss': 0.9509, 'grad_norm': 1.2098324298858643, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.5722070844686649, 'step': 210}, {'loss': 0.9435, 'grad_norm': 1.919457197189331, 'learning_rate': 8.8e-06, 'epoch': 0.5994550408719346, 'step': 220}, {'loss': 0.9364, 'grad_norm': 1.1103708744049072, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.6267029972752044, 'step': 230}, {'loss': 0.9663, 'grad_norm': 1.1000680923461914, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.6539509536784741, 'step': 240}, {'loss': 0.9512, 'grad_norm': 2.2759859561920166, 'learning_rate': 1e-05, 'epoch': 0.6811989100817438, 'step': 250}, {'loss': 0.9485, 'grad_norm': 1.2277756929397583, 'learning_rate': 1.04e-05, 'epoch': 0.7084468664850136, 'step': 260}, {'loss': 0.9478, 'grad_norm': 1.2263147830963135, 'learning_rate': 1.0800000000000002e-05, 'epoch': 0.7356948228882834, 'step': 270}, {'loss': 0.9385, 'grad_norm': 2.0534112453460693, 'learning_rate': 1.1200000000000001e-05, 'epoch': 0.7629427792915532, 'step': 280}, {'loss': 0.954, 'grad_norm': 1.9670372009277344, 'learning_rate': 1.16e-05, 'epoch': 0.7901907356948229, 'step': 290}, {'loss': 0.9353, 'grad_norm': 1.194280743598938, 'learning_rate': 1.2e-05, 'epoch': 0.8174386920980926, 'step': 300}, {'loss': 0.9472, 'grad_norm': 1.2046990394592285, 'learning_rate': 1.2400000000000002e-05, 'epoch': 0.8446866485013624, 'step': 310}, {'loss': 0.9624, 'grad_norm': 1.3865300416946411, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.8719346049046321, 'step': 320}, {'loss': 0.954, 'grad_norm': 1.6100959777832031, 'learning_rate': 1.3200000000000002e-05, 'epoch': 0.8991825613079019, 'step': 330}, {'loss': 0.9483, 'grad_norm': 1.2181596755981445, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.9264305177111717, 'step': 340}, {'loss': 0.9456, 'grad_norm': 1.4008079767227173, 'learning_rate': 1.4e-05, 'epoch': 0.9536784741144414, 'step': 350}, {'loss': 0.9508, 'grad_norm': 1.5381379127502441, 'learning_rate': 1.4400000000000001e-05, 'epoch': 0.9809264305177112, 'step': 360}, {'eval_f1': 0.558925966880813, 'eval_loss': 0.9423011169494918, 'eval_runtime': 24.2833, 'eval_samples_per_second': 60.329, 'eval_steps_per_second': 3.789, 'epoch': 1.0, 'step': 367}, {'loss': 0.9379, 'grad_norm': 1.6401973962783813, 'learning_rate': 1.48e-05, 'epoch': 1.008174386920981, 'step': 370}, {'loss': 0.9507, 'grad_norm': 0.8168703317642212, 'learning_rate': 1.5200000000000002e-05, 'epoch': 1.0354223433242506, 'step': 380}, {'loss': 0.9349, 'grad_norm': 3.8103203773498535, 'learning_rate': 1.5600000000000003e-05, 'epoch': 1.0626702997275204, 'step': 390}, {'loss': 0.9522, 'grad_norm': 1.076136827468872, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.0899182561307903, 'step': 400}, {'loss': 0.9523, 'grad_norm': 1.6345676183700562, 'learning_rate': 1.64e-05, 'epoch': 1.11716621253406, 'step': 410}, {'loss': 0.9658, 'grad_norm': 1.1175941228866577, 'learning_rate': 1.6800000000000002e-05, 'epoch': 1.1444141689373297, 'step': 420}, {'loss': 0.9454, 'grad_norm': 1.0621150732040405, 'learning_rate': 1.72e-05, 'epoch': 1.1716621253405994, 'step': 430}, {'loss': 0.9444, 'grad_norm': 1.7477613687515259, 'learning_rate': 1.76e-05, 'epoch': 1.1989100817438691, 'step': 440}, {'loss': 0.9401, 'grad_norm': 1.5733141899108887, 'learning_rate': 1.8e-05, 'epoch': 1.226158038147139, 'step': 450}, {'loss': 0.9367, 'grad_norm': 0.932148814201355, 'learning_rate': 1.8400000000000003e-05, 'epoch': 1.2534059945504088, 'step': 460}, {'loss': 0.9576, 'grad_norm': 1.466821312904358, 'learning_rate': 1.88e-05, 'epoch': 1.2806539509536785, 'step': 470}, {'loss': 0.9442, 'grad_norm': 1.1372926235198975, 'learning_rate': 1.9200000000000003e-05, 'epoch': 1.3079019073569482, 'step': 480}, {'loss': 0.9419, 'grad_norm': 1.7997444868087769, 'learning_rate': 1.9600000000000002e-05, 'epoch': 1.335149863760218, 'step': 490}, {'loss': 0.9482, 'grad_norm': 0.789256751537323, 'learning_rate': 2e-05, 'epoch': 1.3623978201634879, 'step': 500}, {'loss': 0.9502, 'grad_norm': 1.5278382301330566, 'learning_rate': 1.9850187265917604e-05, 'epoch': 1.3896457765667574, 'step': 510}, {'loss': 0.9616, 'grad_norm': 0.8313583135604858, 'learning_rate': 1.9700374531835207e-05, 'epoch': 1.4168937329700273, 'step': 520}, {'loss': 0.9494, 'grad_norm': 1.844212532043457, 'learning_rate': 1.955056179775281e-05, 'epoch': 1.444141689373297, 'step': 530}, {'loss': 0.9529, 'grad_norm': 2.325863838195801, 'learning_rate': 1.9400749063670416e-05, 'epoch': 1.4713896457765667, 'step': 540}, {'loss': 0.9345, 'grad_norm': 2.5079691410064697, 'learning_rate': 1.925093632958802e-05, 'epoch': 1.4986376021798364, 'step': 550}, {'loss': 0.9541, 'grad_norm': 1.097976803779602, 'learning_rate': 1.910112359550562e-05, 'epoch': 1.5258855585831061, 'step': 560}, {'loss': 0.9398, 'grad_norm': 1.1072925329208374, 'learning_rate': 1.8951310861423224e-05, 'epoch': 1.553133514986376, 'step': 570}, {'loss': 0.9625, 'grad_norm': 2.414320945739746, 'learning_rate': 1.8801498127340823e-05, 'epoch': 1.5803814713896458, 'step': 580}, {'loss': 0.9499, 'grad_norm': 2.7779111862182617, 'learning_rate': 1.8651685393258426e-05, 'epoch': 1.6076294277929155, 'step': 590}, {'loss': 0.9403, 'grad_norm': 1.397125244140625, 'learning_rate': 1.8501872659176032e-05, 'epoch': 1.6348773841961854, 'step': 600}, {'loss': 0.9728, 'grad_norm': 1.2394384145736694, 'learning_rate': 1.8352059925093635e-05, 'epoch': 1.662125340599455, 'step': 610}, {'loss': 0.9598, 'grad_norm': 1.0997698307037354, 'learning_rate': 1.8202247191011237e-05, 'epoch': 1.6893732970027249, 'step': 620}, {'loss': 0.962, 'grad_norm': 1.4053983688354492, 'learning_rate': 1.805243445692884e-05, 'epoch': 1.7166212534059946, 'step': 630}, {'loss': 0.9529, 'grad_norm': 1.1234090328216553, 'learning_rate': 1.7902621722846443e-05, 'epoch': 1.7438692098092643, 'step': 640}, {'loss': 0.9557, 'grad_norm': 0.901324987411499, 'learning_rate': 1.7752808988764045e-05, 'epoch': 1.771117166212534, 'step': 650}, {'loss': 0.9593, 'grad_norm': 0.7851431965827942, 'learning_rate': 1.760299625468165e-05, 'epoch': 1.7983651226158037, 'step': 660}, {'loss': 0.9583, 'grad_norm': 1.0881966352462769, 'learning_rate': 1.7453183520599254e-05, 'epoch': 1.8256130790190737, 'step': 670}, {'loss': 0.96, 'grad_norm': 1.3101806640625, 'learning_rate': 1.7303370786516857e-05, 'epoch': 1.8528610354223434, 'step': 680}, {'loss': 0.9526, 'grad_norm': 1.4921914339065552, 'learning_rate': 1.715355805243446e-05, 'epoch': 1.880108991825613, 'step': 690}, {'loss': 0.9619, 'grad_norm': 2.0586957931518555, 'learning_rate': 1.7003745318352062e-05, 'epoch': 1.9073569482288828, 'step': 700}, {'loss': 0.953, 'grad_norm': 0.8236033916473389, 'learning_rate': 1.6853932584269665e-05, 'epoch': 1.9346049046321525, 'step': 710}, {'loss': 0.936, 'grad_norm': 2.677151679992676, 'learning_rate': 1.6704119850187268e-05, 'epoch': 1.9618528610354224, 'step': 720}, {'loss': 0.9409, 'grad_norm': 0.9700694680213928, 'learning_rate': 1.655430711610487e-05, 'epoch': 1.989100817438692, 'step': 730}, {'eval_f1': 0.558925966880813, 'eval_loss': 0.9447924384569857, 'eval_runtime': 24.2632, 'eval_samples_per_second': 60.379, 'eval_steps_per_second': 3.792, 'epoch': 2.0, 'step': 734}, {'loss': 0.9524, 'grad_norm': 1.5784474611282349, 'learning_rate': 1.6404494382022473e-05, 'epoch': 2.016348773841962, 'step': 740}, {'loss': 0.9616, 'grad_norm': 0.8247445225715637, 'learning_rate': 1.6254681647940076e-05, 'epoch': 2.043596730245232, 'step': 750}, {'loss': 0.9515, 'grad_norm': 1.809335470199585, 'learning_rate': 1.610486891385768e-05, 'epoch': 2.0708446866485013, 'step': 760}, {'loss': 0.9387, 'grad_norm': 1.3700281381607056, 'learning_rate': 1.595505617977528e-05, 'epoch': 2.0980926430517712, 'step': 770}, {'loss': 0.9593, 'grad_norm': 3.12712025642395, 'learning_rate': 1.5805243445692887e-05, 'epoch': 2.1253405994550407, 'step': 780}, {'loss': 0.9551, 'grad_norm': 0.8739835023880005, 'learning_rate': 1.565543071161049e-05, 'epoch': 2.1525885558583107, 'step': 790}, {'loss': 0.9512, 'grad_norm': 2.3168866634368896, 'learning_rate': 1.5505617977528093e-05, 'epoch': 2.1798365122615806, 'step': 800}, {'loss': 0.9442, 'grad_norm': 2.8163154125213623, 'learning_rate': 1.5355805243445695e-05, 'epoch': 2.20708446866485, 'step': 810}, {'loss': 0.9539, 'grad_norm': 2.7694554328918457, 'learning_rate': 1.5205992509363296e-05, 'epoch': 2.23433242506812, 'step': 820}, {'loss': 0.9444, 'grad_norm': 1.183582067489624, 'learning_rate': 1.5056179775280899e-05, 'epoch': 2.2615803814713895, 'step': 830}, {'loss': 0.9481, 'grad_norm': 1.861700415611267, 'learning_rate': 1.4906367041198503e-05, 'epoch': 2.2888283378746594, 'step': 840}, {'loss': 0.937, 'grad_norm': 1.154653549194336, 'learning_rate': 1.4756554307116106e-05, 'epoch': 2.316076294277929, 'step': 850}, {'loss': 0.9606, 'grad_norm': 1.779494047164917, 'learning_rate': 1.4606741573033709e-05, 'epoch': 2.343324250681199, 'step': 860}, {'loss': 0.9433, 'grad_norm': 1.5418106317520142, 'learning_rate': 1.4456928838951311e-05, 'epoch': 2.370572207084469, 'step': 870}, {'loss': 0.9607, 'grad_norm': 1.725132942199707, 'learning_rate': 1.4307116104868914e-05, 'epoch': 2.3978201634877383, 'step': 880}, {'loss': 0.953, 'grad_norm': 1.1460089683532715, 'learning_rate': 1.4157303370786517e-05, 'epoch': 2.4250681198910082, 'step': 890}, {'loss': 0.9496, 'grad_norm': 1.756365180015564, 'learning_rate': 1.4007490636704121e-05, 'epoch': 2.452316076294278, 'step': 900}, {'loss': 0.9388, 'grad_norm': 1.9898440837860107, 'learning_rate': 1.3857677902621724e-05, 'epoch': 2.4795640326975477, 'step': 910}, {'loss': 0.9627, 'grad_norm': 0.9066083431243896, 'learning_rate': 1.3707865168539327e-05, 'epoch': 2.5068119891008176, 'step': 920}, {'loss': 0.956, 'grad_norm': 1.3600573539733887, 'learning_rate': 1.355805243445693e-05, 'epoch': 2.534059945504087, 'step': 930}, {'loss': 0.9508, 'grad_norm': 1.6009631156921387, 'learning_rate': 1.3408239700374532e-05, 'epoch': 2.561307901907357, 'step': 940}, {'loss': 0.9616, 'grad_norm': 1.956830382347107, 'learning_rate': 1.3258426966292135e-05, 'epoch': 2.5885558583106265, 'step': 950}, {'loss': 0.957, 'grad_norm': 1.2905083894729614, 'learning_rate': 1.3108614232209739e-05, 'epoch': 2.6158038147138964, 'step': 960}, {'loss': 0.9447, 'grad_norm': 1.0758188962936401, 'learning_rate': 1.2958801498127342e-05, 'epoch': 2.6430517711171664, 'step': 970}, {'loss': 0.9485, 'grad_norm': 2.9607110023498535, 'learning_rate': 1.2808988764044944e-05, 'epoch': 2.670299727520436, 'step': 980}, {'loss': 0.9535, 'grad_norm': 2.4153406620025635, 'learning_rate': 1.2659176029962547e-05, 'epoch': 2.697547683923706, 'step': 990}, {'loss': 0.9459, 'grad_norm': 1.9200198650360107, 'learning_rate': 1.250936329588015e-05, 'epoch': 2.7247956403269757, 'step': 1000}, {'loss': 0.9547, 'grad_norm': 0.8646265864372253, 'learning_rate': 1.2359550561797752e-05, 'epoch': 2.7520435967302452, 'step': 1010}, {'loss': 0.9662, 'grad_norm': 0.9511563777923584, 'learning_rate': 1.2209737827715359e-05, 'epoch': 2.7792915531335147, 'step': 1020}, {'loss': 0.9415, 'grad_norm': 1.7465629577636719, 'learning_rate': 1.205992509363296e-05, 'epoch': 2.8065395095367847, 'step': 1030}, {'loss': 0.967, 'grad_norm': 1.7338273525238037, 'learning_rate': 1.1910112359550562e-05, 'epoch': 2.8337874659400546, 'step': 1040}, {'loss': 0.9573, 'grad_norm': 2.909822702407837, 'learning_rate': 1.1760299625468165e-05, 'epoch': 2.861035422343324, 'step': 1050}, {'loss': 0.952, 'grad_norm': 2.7392873764038086, 'learning_rate': 1.1610486891385768e-05, 'epoch': 2.888283378746594, 'step': 1060}, {'loss': 0.9412, 'grad_norm': 2.480077028274536, 'learning_rate': 1.146067415730337e-05, 'epoch': 2.915531335149864, 'step': 1070}, {'loss': 0.9431, 'grad_norm': 1.7315351963043213, 'learning_rate': 1.1310861423220976e-05, 'epoch': 2.9427792915531334, 'step': 1080}, {'loss': 0.9373, 'grad_norm': 1.7021458148956299, 'learning_rate': 1.1161048689138579e-05, 'epoch': 2.9700272479564034, 'step': 1090}, {'loss': 0.9562, 'grad_norm': 0.7972358465194702, 'learning_rate': 1.101123595505618e-05, 'epoch': 2.997275204359673, 'step': 1100}, {'eval_f1': 0.558925966880813, 'eval_loss': 0.9421662680487434, 'eval_runtime': 24.2209, 'eval_samples_per_second': 60.485, 'eval_steps_per_second': 3.798, 'epoch': 3.0, 'step': 1101}, {'loss': 0.9704, 'grad_norm': 1.151705026626587, 'learning_rate': 1.0861423220973783e-05, 'epoch': 3.024523160762943, 'step': 1110}, {'loss': 0.936, 'grad_norm': 0.6938866972923279, 'learning_rate': 1.0711610486891385e-05, 'epoch': 3.0517711171662127, 'step': 1120}, {'loss': 0.9414, 'grad_norm': 1.4810409545898438, 'learning_rate': 1.0561797752808988e-05, 'epoch': 3.0790190735694822, 'step': 1130}, {'loss': 0.9488, 'grad_norm': 0.8084993362426758, 'learning_rate': 1.0411985018726594e-05, 'epoch': 3.106267029972752, 'step': 1140}, {'loss': 0.9583, 'grad_norm': 1.6709158420562744, 'learning_rate': 1.0262172284644197e-05, 'epoch': 3.1335149863760217, 'step': 1150}, {'loss': 0.9422, 'grad_norm': 2.0654425621032715, 'learning_rate': 1.01123595505618e-05, 'epoch': 3.1607629427792916, 'step': 1160}, {'loss': 0.9566, 'grad_norm': 1.0360363721847534, 'learning_rate': 9.9625468164794e-06, 'epoch': 3.1880108991825615, 'step': 1170}, {'loss': 0.9444, 'grad_norm': 1.7496428489685059, 'learning_rate': 9.812734082397003e-06, 'epoch': 3.215258855585831, 'step': 1180}, {'loss': 0.9594, 'grad_norm': 1.2219563722610474, 'learning_rate': 9.662921348314608e-06, 'epoch': 3.242506811989101, 'step': 1190}, {'loss': 0.9473, 'grad_norm': 1.1098946332931519, 'learning_rate': 9.51310861423221e-06, 'epoch': 3.2697547683923704, 'step': 1200}, {'loss': 0.9492, 'grad_norm': 0.8151377439498901, 'learning_rate': 9.363295880149813e-06, 'epoch': 3.2970027247956404, 'step': 1210}, {'loss': 0.9316, 'grad_norm': 1.1323223114013672, 'learning_rate': 9.213483146067417e-06, 'epoch': 3.32425068119891, 'step': 1220}, {'loss': 0.9619, 'grad_norm': 2.8573832511901855, 'learning_rate': 9.06367041198502e-06, 'epoch': 3.35149863760218, 'step': 1230}, {'loss': 0.9515, 'grad_norm': 0.9617480039596558, 'learning_rate': 8.913857677902621e-06, 'epoch': 3.3787465940054497, 'step': 1240}, {'loss': 0.936, 'grad_norm': 1.9270542860031128, 'learning_rate': 8.764044943820226e-06, 'epoch': 3.4059945504087192, 'step': 1250}, {'loss': 0.9595, 'grad_norm': 1.9653804302215576, 'learning_rate': 8.614232209737828e-06, 'epoch': 3.433242506811989, 'step': 1260}, {'loss': 0.9427, 'grad_norm': 0.8828631043434143, 'learning_rate': 8.464419475655431e-06, 'epoch': 3.460490463215259, 'step': 1270}, {'loss': 0.9295, 'grad_norm': 1.252462387084961, 'learning_rate': 8.314606741573035e-06, 'epoch': 3.4877384196185286, 'step': 1280}, {'loss': 0.9458, 'grad_norm': 0.8944289684295654, 'learning_rate': 8.164794007490638e-06, 'epoch': 3.5149863760217985, 'step': 1290}, {'loss': 0.9589, 'grad_norm': 1.7894518375396729, 'learning_rate': 8.01498127340824e-06, 'epoch': 3.542234332425068, 'step': 1300}, {'loss': 0.9298, 'grad_norm': 3.3018181324005127, 'learning_rate': 7.865168539325843e-06, 'epoch': 3.569482288828338, 'step': 1310}, {'loss': 0.9591, 'grad_norm': 1.2982332706451416, 'learning_rate': 7.715355805243446e-06, 'epoch': 3.5967302452316074, 'step': 1320}, {'loss': 0.9472, 'grad_norm': 1.4081734418869019, 'learning_rate': 7.565543071161049e-06, 'epoch': 3.6239782016348774, 'step': 1330}, {'loss': 0.9439, 'grad_norm': 0.9205613732337952, 'learning_rate': 7.415730337078652e-06, 'epoch': 3.6512261580381473, 'step': 1340}, {'loss': 0.9567, 'grad_norm': 1.4054639339447021, 'learning_rate': 7.265917602996255e-06, 'epoch': 3.678474114441417, 'step': 1350}, {'loss': 0.9292, 'grad_norm': 1.4104547500610352, 'learning_rate': 7.116104868913858e-06, 'epoch': 3.7057220708446867, 'step': 1360}, {'loss': 0.9535, 'grad_norm': 1.799647569656372, 'learning_rate': 6.966292134831461e-06, 'epoch': 3.7329700272479567, 'step': 1370}, {'loss': 0.9556, 'grad_norm': 1.5826667547225952, 'learning_rate': 6.816479400749064e-06, 'epoch': 3.760217983651226, 'step': 1380}, {'loss': 0.9437, 'grad_norm': 1.737917423248291, 'learning_rate': 6.666666666666667e-06, 'epoch': 3.7874659400544957, 'step': 1390}, {'loss': 0.9586, 'grad_norm': 1.0720734596252441, 'learning_rate': 6.51685393258427e-06, 'epoch': 3.8147138964577656, 'step': 1400}, {'loss': 0.9585, 'grad_norm': 1.420503854751587, 'learning_rate': 6.367041198501873e-06, 'epoch': 3.8419618528610355, 'step': 1410}, {'loss': 0.9478, 'grad_norm': 1.5539056062698364, 'learning_rate': 6.2172284644194756e-06, 'epoch': 3.869209809264305, 'step': 1420}, {'loss': 0.9451, 'grad_norm': 2.490817070007324, 'learning_rate': 6.06741573033708e-06, 'epoch': 3.896457765667575, 'step': 1430}, {'loss': 0.9396, 'grad_norm': 1.2534632682800293, 'learning_rate': 5.917602996254682e-06, 'epoch': 3.923705722070845, 'step': 1440}, {'loss': 0.9488, 'grad_norm': 1.0879225730895996, 'learning_rate': 5.7677902621722845e-06, 'epoch': 3.9509536784741144, 'step': 1450}, {'loss': 0.9735, 'grad_norm': 1.7731369733810425, 'learning_rate': 5.617977528089889e-06, 'epoch': 3.9782016348773843, 'step': 1460}, {'eval_f1': 0.558925966880813, 'eval_loss': 0.9433398793057688, 'eval_runtime': 24.1846, 'eval_samples_per_second': 60.576, 'eval_steps_per_second': 3.804, 'epoch': 4.0, 'step': 1468}, {'loss': 0.9587, 'grad_norm': 1.0356205701828003, 'learning_rate': 5.468164794007491e-06, 'epoch': 4.005449591280654, 'step': 1470}, {'loss': 0.9326, 'grad_norm': 1.4593431949615479, 'learning_rate': 5.318352059925093e-06, 'epoch': 4.032697547683924, 'step': 1480}, {'loss': 0.9435, 'grad_norm': 1.501550316810608, 'learning_rate': 5.168539325842698e-06, 'epoch': 4.059945504087193, 'step': 1490}, {'loss': 0.9157, 'grad_norm': 3.3110318183898926, 'learning_rate': 5.0187265917603005e-06, 'epoch': 4.087193460490464, 'step': 1500}, {'loss': 0.9224, 'grad_norm': 3.6875112056732178, 'learning_rate': 4.868913857677903e-06, 'epoch': 4.114441416893733, 'step': 1510}, {'loss': 0.9395, 'grad_norm': 1.9211463928222656, 'learning_rate': 4.719101123595506e-06, 'epoch': 4.141689373297003, 'step': 1520}, {'loss': 0.8999, 'grad_norm': 4.701286315917969, 'learning_rate': 4.569288389513109e-06, 'epoch': 4.168937329700272, 'step': 1530}, {'loss': 0.9063, 'grad_norm': 8.419708251953125, 'learning_rate': 4.419475655430712e-06, 'epoch': 4.1961852861035425, 'step': 1540}, {'loss': 0.9144, 'grad_norm': 5.922348499298096, 'learning_rate': 4.269662921348315e-06, 'epoch': 4.223433242506812, 'step': 1550}, {'loss': 0.9034, 'grad_norm': 4.450294017791748, 'learning_rate': 4.119850187265918e-06, 'epoch': 4.2506811989100814, 'step': 1560}, {'loss': 0.8451, 'grad_norm': 6.688429832458496, 'learning_rate': 3.970037453183521e-06, 'epoch': 4.277929155313352, 'step': 1570}, {'loss': 0.9144, 'grad_norm': 7.017604827880859, 'learning_rate': 3.820224719101124e-06, 'epoch': 4.305177111716621, 'step': 1580}, {'loss': 0.8636, 'grad_norm': 5.752035140991211, 'learning_rate': 3.670411985018727e-06, 'epoch': 4.332425068119891, 'step': 1590}, {'loss': 0.9335, 'grad_norm': 4.01082706451416, 'learning_rate': 3.52059925093633e-06, 'epoch': 4.359673024523161, 'step': 1600}, {'loss': 0.8928, 'grad_norm': 5.199487686157227, 'learning_rate': 3.3707865168539327e-06, 'epoch': 4.386920980926431, 'step': 1610}, {'loss': 0.9064, 'grad_norm': 8.402616500854492, 'learning_rate': 3.2209737827715358e-06, 'epoch': 4.4141689373297, 'step': 1620}, {'loss': 0.8852, 'grad_norm': 3.5926690101623535, 'learning_rate': 3.0711610486891385e-06, 'epoch': 4.44141689373297, 'step': 1630}, {'loss': 0.9023, 'grad_norm': 6.1294379234313965, 'learning_rate': 2.9213483146067416e-06, 'epoch': 4.46866485013624, 'step': 1640}, {'loss': 0.8847, 'grad_norm': 5.210903644561768, 'learning_rate': 2.771535580524345e-06, 'epoch': 4.4959128065395095, 'step': 1650}, {'loss': 0.9469, 'grad_norm': 5.737252235412598, 'learning_rate': 2.6217228464419474e-06, 'epoch': 4.523160762942779, 'step': 1660}, {'loss': 0.9272, 'grad_norm': 4.352263450622559, 'learning_rate': 2.4719101123595505e-06, 'epoch': 4.550408719346049, 'step': 1670}, {'loss': 0.8874, 'grad_norm': 5.658309459686279, 'learning_rate': 2.322097378277154e-06, 'epoch': 4.577656675749319, 'step': 1680}, {'loss': 0.8829, 'grad_norm': 5.821322441101074, 'learning_rate': 2.1722846441947567e-06, 'epoch': 4.604904632152588, 'step': 1690}, {'loss': 0.9057, 'grad_norm': 3.9087717533111572, 'learning_rate': 2.02247191011236e-06, 'epoch': 4.632152588555858, 'step': 1700}, {'loss': 0.8404, 'grad_norm': 5.839541435241699, 'learning_rate': 1.8726591760299627e-06, 'epoch': 4.659400544959128, 'step': 1710}, {'loss': 0.9057, 'grad_norm': 6.758647441864014, 'learning_rate': 1.7228464419475657e-06, 'epoch': 4.686648501362398, 'step': 1720}, {'loss': 0.8884, 'grad_norm': 5.259434700012207, 'learning_rate': 1.5730337078651686e-06, 'epoch': 4.713896457765667, 'step': 1730}, {'loss': 0.871, 'grad_norm': 4.458516597747803, 'learning_rate': 1.4232209737827715e-06, 'epoch': 4.741144414168938, 'step': 1740}, {'loss': 0.9193, 'grad_norm': 7.230118751525879, 'learning_rate': 1.2734082397003748e-06, 'epoch': 4.768392370572207, 'step': 1750}, {'loss': 0.9247, 'grad_norm': 8.982732772827148, 'learning_rate': 1.1235955056179777e-06, 'epoch': 4.795640326975477, 'step': 1760}, {'loss': 0.9197, 'grad_norm': 7.0980916023254395, 'learning_rate': 9.737827715355806e-07, 'epoch': 4.822888283378747, 'step': 1770}, {'loss': 0.8601, 'grad_norm': 4.6276116371154785, 'learning_rate': 8.239700374531835e-07, 'epoch': 4.8501362397820165, 'step': 1780}, {'loss': 0.898, 'grad_norm': 6.1848955154418945, 'learning_rate': 6.741573033707865e-07, 'epoch': 4.877384196185286, 'step': 1790}, {'loss': 0.9003, 'grad_norm': 5.134937286376953, 'learning_rate': 5.243445692883896e-07, 'epoch': 4.904632152588556, 'step': 1800}, {'loss': 0.8869, 'grad_norm': 4.4207000732421875, 'learning_rate': 3.7453183520599253e-07, 'epoch': 4.931880108991826, 'step': 1810}, {'loss': 0.8825, 'grad_norm': 7.083925247192383, 'learning_rate': 2.247191011235955e-07, 'epoch': 4.959128065395095, 'step': 1820}, {'loss': 0.8686, 'grad_norm': 2.83551025390625, 'learning_rate': 7.490636704119851e-08, 'epoch': 4.986376021798365, 'step': 1830}, {'eval_f1': 0.6366125307032929, 'eval_loss': 1.0004376671884794, 'eval_runtime': 23.7223, 'eval_samples_per_second': 61.756, 'eval_steps_per_second': 3.878, 'epoch': 5.0, 'step': 1835}, {'train_runtime': 2428.5908, 'train_samples_per_second': 12.058, 'train_steps_per_second': 0.756, 'total_flos': 5330446568937000.0, 'train_loss': 0.9409675943754024, 'epoch': 5.0, 'step': 1835}, {'eval_f1': 0.6366125307032929, 'eval_loss': 1.0004376671884794, 'eval_runtime': 23.6902, 'eval_samples_per_second': 61.84, 'eval_steps_per_second': 3.883, 'epoch': 5.0, 'step': 1835}, {'eval_f1': 0.6366125307032929, 'eval_loss': 1.0004376671884794, 'eval_runtime': 23.9309, 'eval_samples_per_second': 61.218, 'eval_steps_per_second': 3.844, 'epoch': 5.0, 'step': 1835}]\n",
      "Number of training steps: 183\n",
      "Number of evaluation steps: 7\n",
      "\n",
      "Train Loss:\n",
      "[0.9628, 0.9652, 0.9642, 0.9614, 0.9633, 0.9623, 0.9639, 0.9505, 0.957, 0.9547, 0.9582, 0.9525, 0.9458, 0.9493, 0.9556, 0.9509, 0.9628, 0.9438, 0.9598, 0.948, 0.9509, 0.9435, 0.9364, 0.9663, 0.9512, 0.9485, 0.9478, 0.9385, 0.954, 0.9353, 0.9472, 0.9624, 0.954, 0.9483, 0.9456, 0.9508, 0.9379, 0.9507, 0.9349, 0.9522, 0.9523, 0.9658, 0.9454, 0.9444, 0.9401, 0.9367, 0.9576, 0.9442, 0.9419, 0.9482, 0.9502, 0.9616, 0.9494, 0.9529, 0.9345, 0.9541, 0.9398, 0.9625, 0.9499, 0.9403, 0.9728, 0.9598, 0.962, 0.9529, 0.9557, 0.9593, 0.9583, 0.96, 0.9526, 0.9619, 0.953, 0.936, 0.9409, 0.9524, 0.9616, 0.9515, 0.9387, 0.9593, 0.9551, 0.9512, 0.9442, 0.9539, 0.9444, 0.9481, 0.937, 0.9606, 0.9433, 0.9607, 0.953, 0.9496, 0.9388, 0.9627, 0.956, 0.9508, 0.9616, 0.957, 0.9447, 0.9485, 0.9535, 0.9459, 0.9547, 0.9662, 0.9415, 0.967, 0.9573, 0.952, 0.9412, 0.9431, 0.9373, 0.9562, 0.9704, 0.936, 0.9414, 0.9488, 0.9583, 0.9422, 0.9566, 0.9444, 0.9594, 0.9473, 0.9492, 0.9316, 0.9619, 0.9515, 0.936, 0.9595, 0.9427, 0.9295, 0.9458, 0.9589, 0.9298, 0.9591, 0.9472, 0.9439, 0.9567, 0.9292, 0.9535, 0.9556, 0.9437, 0.9586, 0.9585, 0.9478, 0.9451, 0.9396, 0.9488, 0.9735, 0.9587, 0.9326, 0.9435, 0.9157, 0.9224, 0.9395, 0.8999, 0.9063, 0.9144, 0.9034, 0.8451, 0.9144, 0.8636, 0.9335, 0.8928, 0.9064, 0.8852, 0.9023, 0.8847, 0.9469, 0.9272, 0.8874, 0.8829, 0.9057, 0.8404, 0.9057, 0.8884, 0.871, 0.9193, 0.9247, 0.9197, 0.8601, 0.898, 0.9003, 0.8869, 0.8825, 0.8686]\n",
      "\n",
      "Eval Loss:\n",
      "[0.9423011169494918, 0.9447924384569857, 0.9421662680487434, 0.9433398793057688, 1.0004376671884794, 1.0004376671884794, 1.0004376671884794]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# Evaluate the model and manually calculate metrics\n",
    "print(\"\\nEvaluating the model and calculating metrics...\")\n",
    "test_results = trainer.predict(test_dataset)\n",
    "test_preds = (torch.sigmoid(torch.Tensor(test_results.predictions)) > 0.5).float().numpy()\n",
    "test_labels = test_dataset.labels\n",
    "\n",
    "accuracy = accuracy_score(test_labels, test_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_preds, average='weighted')\n",
    "\n",
    "print(\"\\nTest set results:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Print all available metrics from the evaluation\n",
    "print(\"\\nAll computed metrics from evaluate():\")\n",
    "eval_result = trainer.evaluate()\n",
    "for key, value in eval_result.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Update the plotting code to use the correct metric names\n",
    "history = trainer.state.log_history\n",
    "print(\"\\nTraining history:\")\n",
    "print(history)\n",
    "\n",
    "# Extract loss values\n",
    "train_loss = [x['loss'] for x in history if 'loss' in x and 'eval_loss' not in x]\n",
    "eval_loss = [x['eval_loss'] for x in history if 'eval_loss' in x]\n",
    "\n",
    "# Create x-axis values\n",
    "train_steps = np.arange(len(train_loss))\n",
    "eval_steps = np.linspace(0, len(train_loss) - 1, num=len(eval_loss))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(train_steps, train_loss, label='Train Loss')\n",
    "plt.plot(eval_steps, eval_loss, label='Eval Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Evaluation Loss')\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png')\n",
    "plt.close()\n",
    "\n",
    "print(f\"Number of training steps: {len(train_loss)}\")\n",
    "print(f\"Number of evaluation steps: {len(eval_loss)}\")\n",
    "\n",
    "# If you want to see the actual loss values\n",
    "print(\"\\nTrain Loss:\")\n",
    "print(train_loss)\n",
    "print(\"\\nEval Loss:\")\n",
    "print(eval_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating confusion matrices...\n",
      "\n",
      "Per-class metrics:\n",
      "\n",
      "Class biomistral:\n",
      "Precision: 0.6328\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.7751\n",
      "\n",
      "Class medalpaca:\n",
      "Precision: 0.5904\n",
      "Recall: 0.6018\n",
      "F1 Score: 0.5961\n",
      "\n",
      "Class meditron:\n",
      "Precision: 0.4490\n",
      "Recall: 0.5539\n",
      "F1 Score: 0.4959\n",
      "\n",
      "Prediction distribution:\n",
      "biomistral: 1465.0 predictions (100.00%)\n",
      "medalpaca: 896.0 predictions (61.16%)\n",
      "meditron: 813.0 predictions (55.49%)\n"
     ]
    }
   ],
   "source": [
    "# Add confusion matrix\n",
    "print(\"\\nGenerating confusion matrices...\")\n",
    "for i in range(test_preds.shape[1]):\n",
    "    cm = confusion_matrix(test_labels[:, i], test_preds[:, i])\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for Class {mlb.classes_[i]}')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(f'confusion_matrix_class_{mlb.classes_[i]}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Calculate and print per-class metrics\n",
    "print(\"\\nPer-class metrics:\")\n",
    "for i, class_name in enumerate(mlb.classes_):\n",
    "    class_precision, class_recall, class_f1, _ = precision_recall_fscore_support(test_labels[:, i], test_preds[:, i], average='binary')\n",
    "    print(f\"\\nClass {class_name}:\")\n",
    "    print(f\"Precision: {class_precision:.4f}\")\n",
    "    print(f\"Recall: {class_recall:.4f}\")\n",
    "    print(f\"F1 Score: {class_f1:.4f}\")\n",
    "\n",
    "# Analyze prediction distribution\n",
    "print(\"\\nPrediction distribution:\")\n",
    "pred_counts = test_preds.sum(axis=0)\n",
    "for i, class_name in enumerate(mlb.classes_):\n",
    "    print(f\"{class_name}: {pred_counts[i]} predictions ({pred_counts[i]/len(test_preds)*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for 10 random samples:\n",
      "\n",
      "Sample 1:\n",
      "Text: the key medical concepts being tested in this question are chronic venous insufficiency, deep vein t...\n",
      "Predicted labels: ['biomistral' 'meditron']\n",
      "True labels: ['biomistral' 'meditron']\n",
      "Probabilities for each label:\n",
      "biomistral: 0.6089\n",
      "medalpaca: 0.4835\n",
      "meditron: 0.6231\n",
      "Sample accuracy: 1.00\n",
      "\n",
      "Sample 2:\n",
      "Text: the key medical concepts being tested in this question are the pharmacodynamics and potential advers...\n",
      "Predicted labels: ['biomistral' 'medalpaca' 'meditron']\n",
      "True labels: ['medalpaca']\n",
      "Probabilities for each label:\n",
      "biomistral: 0.6372\n",
      "medalpaca: 0.5616\n",
      "meditron: 0.5128\n",
      "Sample accuracy: 0.33\n",
      "\n",
      "Sample 3:\n",
      "Text: the key medical concepts being tested in this question include: seizure types and classification, se...\n",
      "Predicted labels: ['biomistral' 'meditron']\n",
      "True labels: ['biomistral' 'medalpaca']\n",
      "Probabilities for each label:\n",
      "biomistral: 0.6234\n",
      "medalpaca: 0.4970\n",
      "meditron: 0.6686\n",
      "Sample accuracy: 0.33\n",
      "\n",
      "Sample 4:\n",
      "Text: the key medical concepts being tested in this question are the risk factors, genetic predisposition,...\n",
      "Predicted labels: ['biomistral' 'meditron']\n",
      "True labels: ['biomistral' 'medalpaca' 'meditron']\n",
      "Probabilities for each label:\n",
      "biomistral: 0.6058\n",
      "medalpaca: 0.4670\n",
      "meditron: 0.6942\n",
      "Sample accuracy: 0.67\n",
      "\n",
      "Sample 5:\n",
      "Text: the key medical concepts being tested are postpartum endometritis, lactation, and post-delivery cont...\n",
      "Predicted labels: ['biomistral' 'meditron']\n",
      "True labels: ['biomistral' 'meditron']\n",
      "Probabilities for each label:\n",
      "biomistral: 0.6090\n",
      "medalpaca: 0.4498\n",
      "meditron: 0.6934\n",
      "Sample accuracy: 1.00\n",
      "\n",
      "Sample 6:\n",
      "Text: the medical concepts being tested in this question are the characteristics of a heart murmur, specif...\n",
      "Predicted labels: ['biomistral' 'meditron']\n",
      "True labels: ['medalpaca']\n",
      "Probabilities for each label:\n",
      "biomistral: 0.6046\n",
      "medalpaca: 0.4451\n",
      "meditron: 0.6683\n",
      "Sample accuracy: 0.00\n",
      "\n",
      "Sample 7:\n",
      "Text: the key medical concept being tested in this question is the role of apocrine glands in producing a ...\n",
      "Predicted labels: ['biomistral' 'meditron']\n",
      "True labels: ['biomistral' 'medalpaca']\n",
      "Probabilities for each label:\n",
      "biomistral: 0.6215\n",
      "medalpaca: 0.4785\n",
      "meditron: 0.6618\n",
      "Sample accuracy: 0.33\n",
      "\n",
      "Sample 8:\n",
      "Text: the key medical concepts being tested in this question are the potential effects of childhood pylori...\n",
      "Predicted labels: ['biomistral' 'medalpaca' 'meditron']\n",
      "True labels: ['biomistral' 'meditron']\n",
      "Probabilities for each label:\n",
      "biomistral: 0.6087\n",
      "medalpaca: 0.5454\n",
      "meditron: 0.6163\n",
      "Sample accuracy: 0.67\n",
      "\n",
      "Sample 9:\n",
      "Text: the key medical concepts being tested in this question are the morphological changes that occur in m...\n",
      "Predicted labels: ['biomistral' 'meditron']\n",
      "True labels: ['medalpaca']\n",
      "Probabilities for each label:\n",
      "biomistral: 0.6163\n",
      "medalpaca: 0.4277\n",
      "meditron: 0.6628\n",
      "Sample accuracy: 0.00\n",
      "\n",
      "Sample 10:\n",
      "Text: the medical concepts being tested in this question include the importance of medication safety, part...\n",
      "Predicted labels: ['biomistral' 'medalpaca' 'meditron']\n",
      "True labels: ['biomistral' 'medalpaca']\n",
      "Probabilities for each label:\n",
      "biomistral: 0.6442\n",
      "medalpaca: 0.5688\n",
      "meditron: 0.5455\n",
      "Sample accuracy: 0.67\n",
      "\n",
      "Overall metrics for the random sample:\n",
      "Accuracy: 0.2000\n",
      "Precision: 0.6204\n",
      "Recall: 0.7222\n",
      "F1 Score: 0.6028\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def predict(texts, threshold=0.5):\n",
    "    # Get the device of the model\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Tokenize and move to the correct device\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "    encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "    \n",
    "    # Move logits to CPU for numpy conversion\n",
    "    logits = outputs.logits.cpu()\n",
    "    probabilities = torch.sigmoid(logits).numpy()\n",
    "    predictions = (probabilities > threshold).astype(int)\n",
    "    \n",
    "    # Ensure predictions match the number of classes in mlb\n",
    "    if predictions.shape[1] != len(mlb.classes_):\n",
    "        # Pad or truncate predictions to match mlb.classes_\n",
    "        new_predictions = np.zeros((predictions.shape[0], len(mlb.classes_)))\n",
    "        new_probabilities = np.zeros((probabilities.shape[0], len(mlb.classes_)))\n",
    "        min_classes = min(predictions.shape[1], len(mlb.classes_))\n",
    "        new_predictions[:, :min_classes] = predictions[:, :min_classes]\n",
    "        new_probabilities[:, :min_classes] = probabilities[:, :min_classes]\n",
    "        predictions = new_predictions\n",
    "        probabilities = new_probabilities\n",
    "    \n",
    "    return predictions, probabilities\n",
    "\n",
    "def predict_random_sample(texts, labels, sample_size=10):\n",
    "    # Ensure sample size is not larger than the dataset\n",
    "    sample_size = min(sample_size, len(texts))\n",
    "    \n",
    "    # Randomly sample indices\n",
    "    indices = random.sample(range(len(texts)), sample_size)\n",
    "    \n",
    "    # Get the sampled texts and labels\n",
    "    sampled_texts = [texts[i] for i in indices]\n",
    "    sampled_labels = labels[indices]\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions, probabilities = predict(sampled_texts)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nPredictions for {sample_size} random samples:\")\n",
    "    for i in range(sample_size):\n",
    "        text = sampled_texts[i]\n",
    "        pred = predictions[i]\n",
    "        true = sampled_labels[i]\n",
    "        prob = probabilities[i]\n",
    "        \n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Text: {text[:100]}...\")  # Print first 100 characters of the text\n",
    "        print(f\"Predicted labels: {mlb.classes_[pred.astype(bool)]}\")\n",
    "        print(f\"True labels: {mlb.classes_[true.astype(bool)]}\")\n",
    "        print(\"Probabilities for each label:\")\n",
    "        for label, pb in zip(mlb.classes_, prob):\n",
    "            print(f\"{label}: {pb:.4f}\")\n",
    "        \n",
    "        # Calculate and print accuracy for this sample\n",
    "        sample_accuracy = np.mean(pred == true)\n",
    "        print(f\"Sample accuracy: {sample_accuracy:.2f}\")\n",
    "\n",
    "    # Calculate overall metrics for the sample\n",
    "    sample_accuracy = accuracy_score(sampled_labels, predictions)\n",
    "    sample_precision, sample_recall, sample_f1, _ = precision_recall_fscore_support(sampled_labels, predictions, average='weighted')\n",
    "    \n",
    "    print(\"\\nOverall metrics for the random sample:\")\n",
    "    print(f\"Accuracy: {sample_accuracy:.4f}\")\n",
    "    print(f\"Precision: {sample_precision:.4f}\")\n",
    "    print(f\"Recall: {sample_recall:.4f}\")\n",
    "    print(f\"F1 Score: {sample_f1:.4f}\")\n",
    "\n",
    "# Use the function to predict on a random sample\n",
    "predict_random_sample(test_texts, test_labels, sample_size=10)\n",
    "\n",
    "# If you want to analyze the prediction distribution for the entire test set\n",
    "print(\"\\nPrediction distribution for the entire test set:\")\n",
    "all_predictions, _ = predict(test_texts)\n",
    "pred_counts = all_predictions.sum(axis=0)\n",
    "for i, class_name in enumerate(mlb.classes_):\n",
    "    print(f\"{class_name}: {pred_counts[i]} predictions ({pred_counts[i]/len(all_predictions)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom threhsold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_threshold\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Assuming you have a validation set\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m val_predictions, val_probabilities \u001b[38;5;241m=\u001b[39m predict(\u001b[43mval_texts\u001b[49m)\n\u001b[1;32m     12\u001b[0m optimal_thresholds \u001b[38;5;241m=\u001b[39m find_optimal_threshold(val_labels, val_probabilities)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_with_custom_threshold\u001b[39m(texts, thresholds):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_texts' is not defined"
     ]
    }
   ],
   "source": [
    "# Replace the existing find_optimal_threshold function with this new one\n",
    "def find_optimal_threshold(y_true, y_pred):\n",
    "    best_threshold = np.zeros(y_pred.shape[1])\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true[:, i], y_pred[:, i])\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "        best_threshold[i] = thresholds[np.argmax(f1_scores)]\n",
    "    return best_threshold\n",
    "\n",
    "#create a validation set\n",
    "\n",
    "# Assuming you have a validation set\n",
    "val_predictions, val_probabilities = predict(val_texts)\n",
    "optimal_thresholds = find_optimal_threshold(val_labels, val_probabilities)\n",
    "\n",
    "def predict_with_custom_threshold(texts, thresholds):\n",
    "    _, probabilities = predict(texts)\n",
    "    predictions = (probabilities > thresholds).astype(int)\n",
    "    return predictions, probabilities\n",
    "\n",
    "def predict_random_sample(texts, labels, sample_size=10):\n",
    "    # Ensure sample size is not larger than the dataset\n",
    "    sample_size = min(sample_size, len(texts))\n",
    "    \n",
    "    # Randomly sample indices\n",
    "    indices = random.sample(range(len(texts)), sample_size)\n",
    "    \n",
    "    # Get the sampled texts and labels\n",
    "    sampled_texts = [texts[i] for i in indices]\n",
    "    sampled_labels = labels[indices]\n",
    "    \n",
    "    # Make predictions using custom thresholds\n",
    "    predictions, probabilities = predict_with_custom_threshold(sampled_texts, optimal_thresholds)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nPredictions for {sample_size} random samples:\")\n",
    "    for i in range(sample_size):\n",
    "        text = sampled_texts[i]\n",
    "        pred = predictions[i]\n",
    "        true = sampled_labels[i]\n",
    "        prob = probabilities[i]\n",
    "        \n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Text: {text[:100]}...\")  # Print first 100 characters of the text\n",
    "        print(f\"Predicted labels: {mlb.classes_[pred.astype(bool)]}\")\n",
    "        print(f\"True labels: {mlb.classes_[true.astype(bool)]}\")\n",
    "        print(\"Probabilities for each label:\")\n",
    "        for label, pb, threshold in zip(mlb.classes_, prob, optimal_thresholds):\n",
    "            print(f\"{label}: {pb:.4f} (threshold: {threshold:.2f})\")\n",
    "        \n",
    "        # Calculate and print accuracy for this sample\n",
    "        sample_accuracy = np.mean(pred == true)\n",
    "        print(f\"Sample accuracy: {sample_accuracy:.2f}\")\n",
    "\n",
    "    # Calculate overall metrics for the sample\n",
    "    sample_accuracy = accuracy_score(sampled_labels, predictions)\n",
    "    sample_precision, sample_recall, sample_f1, _ = precision_recall_fscore_support(sampled_labels, predictions, average='weighted')\n",
    "    \n",
    "    print(\"\\nOverall metrics for the random sample:\")\n",
    "    print(f\"Accuracy: {sample_accuracy:.4f}\")\n",
    "    print(f\"Precision: {sample_precision:.4f}\")\n",
    "    print(f\"Recall: {sample_recall:.4f}\")\n",
    "    print(f\"F1 Score: {sample_f1:.4f}\")\n",
    "\n",
    "# Use the function to predict on a random sample\n",
    "predict_random_sample(test_texts, test_labels, sample_size=10)\n",
    "\n",
    "# Analyze prediction distribution for the entire test set\n",
    "print(\"\\nPrediction distribution for the entire test set:\")\n",
    "all_predictions, _ = predict_with_custom_threshold(test_texts, optimal_thresholds)\n",
    "pred_counts = all_predictions.sum(axis=0)\n",
    "for i, class_name in enumerate(mlb.classes_):\n",
    "    print(f\"{class_name}: {pred_counts[i]} predictions ({pred_counts[i]/len(all_predictions)*100:.2f}%)\")\n",
    "\n",
    "# Print class distribution in the true labels\n",
    "print(\"\\nTrue label distribution:\")\n",
    "true_counts = test_labels.sum(axis=0)\n",
    "for i, class_name in enumerate(mlb.classes_):\n",
    "    print(f\"{class_name}: {true_counts[i]} occurrences ({true_counts[i]/len(test_labels)*100:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
