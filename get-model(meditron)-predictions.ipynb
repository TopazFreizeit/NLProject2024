{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniforge3/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:411: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):\n",
      "/home/ubuntu/miniforge3/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:419: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/ubuntu/miniforge3/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6724c6891f8643a7b16c737360308c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniforge3/lib/python3.10/site-packages/transformers/modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at TheBloke/meditron-7B-GPTQ were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fb040dde5e445795e3efcd01bd7a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit = True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/meditron-7B-GPTQ\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/meditron-7B-GPTQ\", device_map='auto')\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/home/ubuntu/nlp/local_meditron_model\")\n",
    "# model = AutoModelForCausalLM.from_pretrained('/home/ubuntu/nlp/local_meditron_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LlamaTokenizerFast(name_or_path='TheBloke/meditron-7B-GPTQ', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '<SEP>', 'pad_token': '<PAD>', 'cls_token': '<CLS>', 'mask_token': '<MASK>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       " \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t32000: AddedToken(\"<CLS>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t32001: AddedToken(\"<SEP>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t32002: AddedToken(\"<EOD>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t32003: AddedToken(\"<MASK>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t32004: AddedToken(\"<PAD>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " },\n",
       " LlamaForCausalLM(\n",
       "   (model): LlamaModel(\n",
       "     (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "     (layers): ModuleList(\n",
       "       (0-31): 32 x LlamaDecoderLayer(\n",
       "         (self_attn): LlamaSdpaAttention(\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "           (k_proj): QuantLinear()\n",
       "           (o_proj): QuantLinear()\n",
       "           (q_proj): QuantLinear()\n",
       "           (v_proj): QuantLinear()\n",
       "         )\n",
       "         (mlp): LlamaMLP(\n",
       "           (act_fn): SiLU()\n",
       "           (down_proj): QuantLinear()\n",
       "           (gate_proj): QuantLinear()\n",
       "           (up_proj): QuantLinear()\n",
       "         )\n",
       "         (input_layernorm): LlamaRMSNorm()\n",
       "         (post_attention_layernorm): LlamaRMSNorm()\n",
       "       )\n",
       "     )\n",
       "     (norm): LlamaRMSNorm()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       " ))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer, model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./local_meditron_model/tokenizer_config.json',\n",
       " './local_meditron_model/special_tokens_map.json',\n",
       " './local_meditron_model/tokenizer.model',\n",
       " './local_meditron_model/added_tokens.json',\n",
       " './local_meditron_model/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./local_meditron_model\")\n",
    "tokenizer.save_pretrained(\"./local_meditron_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are multiple choice questions (with answers) about medical:\n",
      "\n",
      "Question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\n",
      "Choices:\n",
      "A. Ampicillin\n",
      "B. Ceftriaxone\n",
      "C. Doxycycline\n",
      "D. Nitrofurantoin\n",
      "Correct Answer:D\n",
      "\n",
      "Question: A 3-month-old baby died suddenly at night while asleep. His mother noticed that he had died only after she awoke in the morning. No cause of death was determined based on the autopsy. Which of the following precautions could have prevented the death of the baby?\n",
      "Choices:\n",
      "A. Placing the infant in a supine position on a firm mattress while sleeping\n",
      "B. Keeping the infant covered and maintaining a high room temperature\n",
      "C. Application of a device to maintain the sleeping position\n",
      "D. Avoiding pacifier use during sleep\n",
      "Correct Answer:A\n",
      "\n",
      "Question: A mother brings her 3-week-old infant to the pediatrician's office because she is concerned about his feeding habits. He was born without complications and has not had any medical problems up until this time. However, for the past 4 days, he has been fussy, is regurgitating all of his feeds, and his vomit is yellow in color. On physical exam, the child's abdomen is minimally distended but no other abnormalities are appreciated. Which of the following embryologic errors could account for this presentation?\n",
      "Choices:\n",
      "A. Abnormal migration of ventral pancreatic bud\n",
      "B. Complete failure of proximal duodenum to recanalize\n",
      "C. Abnormal hypertrophy of the pylorus\n",
      "D. Failure of lateral body folds to move ventrally and fuse in the midline\n",
      "Correct Answer:A\n",
      "\n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx', 'metamap_phrases'],\n",
      "        num_rows: 10175\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx', 'metamap_phrases'],\n",
      "        num_rows: 1273\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "from datasets import load_from_disk\n",
    "\n",
    "def load_dataset_and_create_base_prompt(dataset_name = \"MedQA-USMLE-4-options\"):\n",
    "    dataset = load_from_disk(dataset_name)\n",
    "    num_shots = 3     # Number of few-shot examples to use\n",
    "\n",
    "\n",
    "    # Get few-shot examples from the training set\n",
    "    few_shot_examples = dataset['train'].select(range(num_shots))\n",
    "\n",
    "    # remove the few-shot examples from the dataset\n",
    "    dataset['train'] = dataset['train'].select(range(num_shots, len(dataset['train'])))\n",
    "\n",
    "    # Create the prompt with few-shot examples\n",
    "    prompt = \"The following are multiple choice questions (with answers) about medical:\\n\\n\"\n",
    "    for ex in few_shot_examples:\n",
    "        prompt += f\"Question: {ex['question']}\\nChoices:\\n\"\n",
    "        for opt, value in ex['options'].items():\n",
    "            prompt += f\"{opt}. {value}\\n\"\n",
    "        prompt += f\"Correct Answer:{ex['answer_idx']}\\n\\n\"\n",
    "\n",
    "    return dataset, prompt\n",
    "\n",
    "raw_dataset, base_prompt = load_dataset_and_create_base_prompt()\n",
    "print(base_prompt), print(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are multiple choice questions (with answers) about medical:\n",
      "\n",
      "Question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\n",
      "Choices:\n",
      "A. Ampicillin\n",
      "B. Ceftriaxone\n",
      "C. Doxycycline\n",
      "D. Nitrofurantoin\n",
      "Correct Answer:D\n",
      "\n",
      "Question: A 3-month-old baby died suddenly at night while asleep. His mother noticed that he had died only after she awoke in the morning. No cause of death was determined based on the autopsy. Which of the following precautions could have prevented the death of the baby?\n",
      "Choices:\n",
      "A. Placing the infant in a supine position on a firm mattress while sleeping\n",
      "B. Keeping the infant covered and maintaining a high room temperature\n",
      "C. Application of a device to maintain the sleeping position\n",
      "D. Avoiding pacifier use during sleep\n",
      "Correct Answer:A\n",
      "\n",
      "Question: A mother brings her 3-week-old infant to the pediatrician's office because she is concerned about his feeding habits. He was born without complications and has not had any medical problems up until this time. However, for the past 4 days, he has been fussy, is regurgitating all of his feeds, and his vomit is yellow in color. On physical exam, the child's abdomen is minimally distended but no other abnormalities are appreciated. Which of the following embryologic errors could account for this presentation?\n",
      "Choices:\n",
      "A. Abnormal migration of ventral pancreatic bud\n",
      "B. Complete failure of proximal duodenum to recanalize\n",
      "C. Abnormal hypertrophy of the pylorus\n",
      "D. Failure of lateral body folds to move ventrally and fuse in the midline\n",
      "Correct Answer:A\n",
      "\n",
      "Question: A pulmonary autopsy specimen from a 58-year-old woman who died of acute hypoxic respiratory failure was examined. She had recently undergone surgery for a fractured femur 3 months ago. Initial hospital course was uncomplicated, and she was discharged to a rehab facility in good health. Shortly after discharge home from rehab, she developed sudden shortness of breath and had cardiac arrest. Resuscitation was unsuccessful. On histological examination of lung tissue, fibrous connective tissue around the lumen of the pulmonary artery is observed. Which of the following is the most likely pathogenesis for the present findings?\n",
      "Choices:\n",
      "A. Thromboembolism\n",
      "B. Pulmonary ischemia\n",
      "C. Pulmonary hypertension\n",
      "D. Pulmonary passive congestion\n",
      "Correct Answer:\n",
      "A\n",
      "{'question': 'A pulmonary autopsy specimen from a 58-year-old woman who died of acute hypoxic respiratory failure was examined. She had recently undergone surgery for a fractured femur 3 months ago. Initial hospital course was uncomplicated, and she was discharged to a rehab facility in good health. Shortly after discharge home from rehab, she developed sudden shortness of breath and had cardiac arrest. Resuscitation was unsuccessful. On histological examination of lung tissue, fibrous connective tissue around the lumen of the pulmonary artery is observed. Which of the following is the most likely pathogenesis for the present findings?', 'answer': 'Thromboembolism', 'options': {'A': 'Thromboembolism', 'B': 'Pulmonary ischemia', 'C': 'Pulmonary hypertension', 'D': 'Pulmonary passive congestion'}, 'meta_info': 'step1', 'answer_idx': 'A', 'metamap_phrases': ['pulmonary autopsy specimen', '58 year old woman', 'died', 'acute hypoxic respiratory failure', 'examined', 'recently', 'surgery', 'fractured femur 3 months', 'Initial hospital course', 'uncomplicated', 'discharged', 'rehab facility', 'good health', 'discharge home', 'rehab', 'sudden shortness of breath', 'cardiac', 'Resuscitation', 'unsuccessful', 'histological examination', 'lung tissue', 'fibrous connective tissue', 'lumen of', 'pulmonary artery', 'observed', 'following', 'most likely pathogenesis', 'present findings']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_prompt_and_return_correct_answer(item, prompt=base_prompt):\n",
    "    # add to the prompt the first question from the dataset\n",
    "    prompt += f\"Question: {item['question']}\\nChoices:\\n\"\n",
    "    for opt, value in item['options'].items():\n",
    "        prompt += f\"{opt}. {value}\\n\"\n",
    "    correct_answer_letter = item['answer_idx']\n",
    "    # empty answer\n",
    "    prompt += f\"Correct Answer:\"    \n",
    "    # print(f\"finale prompt to be sent to model is: {prompt}\")\n",
    "    # tokenize the prompt\n",
    "    return prompt, correct_answer_letter\n",
    "\n",
    "example_prompt, example_answer = create_prompt_and_return_correct_answer(raw_dataset['train'][0], base_prompt)\n",
    "print(example_prompt), print(example_answer), print(raw_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "DEBUG:root:top 10 tokens with probabilities:\n",
      "DEBUG:root:0.3108: \"C\"\n",
      "DEBUG:root:0.2383: \"B\"\n",
      "DEBUG:root:0.2238: \"D\"\n",
      "DEBUG:root:0.2038: \"A\"\n",
      "DEBUG:root:0.0046: \"▁A\"\n",
      "DEBUG:root:0.0039: \"▁C\"\n",
      "DEBUG:root:0.0039: \"▁B\"\n",
      "DEBUG:root:0.0038: \"▁D\"\n",
      "DEBUG:root:0.0031: \"E\"\n",
      "DEBUG:root:0.0030: \"<0x0A>\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0.20379683375358582, 'B': 0.23826263844966888, 'C': 0.3107530474662781, 'D': 0.2238270342350006}\n",
      "C\n",
      "A\n",
      "0.20379683375358582\n",
      "{'question': 'A pulmonary autopsy specimen from a 58-year-old woman who died of acute hypoxic respiratory failure was examined. She had recently undergone surgery for a fractured femur 3 months ago. Initial hospital course was uncomplicated, and she was discharged to a rehab facility in good health. Shortly after discharge home from rehab, she developed sudden shortness of breath and had cardiac arrest. Resuscitation was unsuccessful. On histological examination of lung tissue, fibrous connective tissue around the lumen of the pulmonary artery is observed. Which of the following is the most likely pathogenesis for the present findings?', 'answer': 'Thromboembolism', 'options': {'A': 'Thromboembolism', 'B': 'Pulmonary ischemia', 'C': 'Pulmonary hypertension', 'D': 'Pulmonary passive congestion'}, 'meta_info': 'step1', 'answer_idx': 'A', 'metamap_phrases': ['pulmonary autopsy specimen', '58 year old woman', 'died', 'acute hypoxic respiratory failure', 'examined', 'recently', 'surgery', 'fractured femur 3 months', 'Initial hospital course', 'uncomplicated', 'discharged', 'rehab facility', 'good health', 'discharge home', 'rehab', 'sudden shortness of breath', 'cardiac', 'Resuscitation', 'unsuccessful', 'histological examination', 'lung tissue', 'fibrous connective tissue', 'lumen of', 'pulmonary artery', 'observed', 'following', 'most likely pathogenesis', 'present findings']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None, None, None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "def get_probabilities(prompt):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # encode the prompt\n",
    "        prompt = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # generate the answer to get the logits\n",
    "        logits = model(**prompt).logits\n",
    "\n",
    "        # only care about the last projection in the last batch\n",
    "        logits = logits[-1, -1].clone().detach().cpu()\n",
    "\n",
    "\n",
    "        # softmax() to get probabilities\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        \n",
    "        # dict of probabilities with token names\n",
    "        probs, ids = torch.sort(probs, descending=True)\n",
    "\n",
    "        # keep only the top k\n",
    "        k = 10\n",
    "        probs_top_k = probs[:k]\n",
    "        ids_top_k = ids[:k]\n",
    "        texts_top_k = tokenizer.convert_ids_to_tokens(ids_top_k)\n",
    "\n",
    "        # print\n",
    "        logging.debug(f\"top {k} tokens with probabilities:\")\n",
    "        for prob, text in zip(probs_top_k, texts_top_k):\n",
    "            logging.debug(f\"{prob:.4f}: \\\"{text}\\\"\")\n",
    "        \n",
    "        # make the key value in probs\n",
    "        probs_name_list = [(probs[i].item(), tokenizer.convert_ids_to_tokens(ids[i].item())) for i in range(len(probs))]\n",
    "        \n",
    "        # make probs_name_value a list of tuples to be dictionary\n",
    "        probs_name_dict = dict(probs_name_list)\n",
    "        \n",
    "        # switch the key and value\n",
    "        name_probs_dict = {v: k for k, v in probs_name_dict.items()}\n",
    "                \n",
    "        # if A, B, C or D not exist set probability to 0\n",
    "        for letter in ['A', 'B', 'C', 'D']:\n",
    "            if letter not in name_probs_dict:\n",
    "                name_probs_dict[letter] = 0.0\n",
    "\n",
    "        return {\"A\": name_probs_dict['A'], \"B\": name_probs_dict['B'], \"C\": name_probs_dict['C'], \"D\": name_probs_dict['D']}\n",
    "\n",
    "\n",
    "example_probabilities = get_probabilities(example_prompt)\n",
    "\n",
    "# get the answer with the highest probability\n",
    "answer_with_max_prob = max(example_probabilities, key=example_probabilities.get)\n",
    "\n",
    "# get the correct answer probability\n",
    "correct_answer_prob = example_probabilities[example_answer]\n",
    "\n",
    "print(example_probabilities), print(answer_with_max_prob), print(example_answer), print(correct_answer_prob), print(raw_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10175 [00:00<?, ?it/s]/tmp/ipykernel_3335/3072891525.py:20: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = df._append({'prompt': prompt, 'correct_answer': correct_answer, 'answer_with_max_prob': answer_with_max_prob, 'prob_of_correct_answer': prob_of_correct_answer, 'probabilities': probabilities}, ignore_index=True)\n",
      "100%|██████████| 10175/10175 [4:34:46<00:00,  1.62s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt correct_answer  \\\n",
      "0  The following are multiple choice questions (w...              A   \n",
      "1  The following are multiple choice questions (w...              D   \n",
      "2  The following are multiple choice questions (w...              C   \n",
      "3  The following are multiple choice questions (w...              D   \n",
      "4  The following are multiple choice questions (w...              A   \n",
      "\n",
      "  answer_with_max_prob  prob_of_correct_answer  \\\n",
      "0                    C                0.203797   \n",
      "1                    C                0.280022   \n",
      "2                    C                0.340576   \n",
      "3                    C                0.288692   \n",
      "4                    C                0.217726   \n",
      "\n",
      "                                       probabilities  \n",
      "0  {'A': 0.20379683375358582, 'B': 0.238262638449...  \n",
      "1  {'A': 0.14080394804477692, 'B': 0.211371764540...  \n",
      "2  {'A': 0.17668865621089935, 'B': 0.168597489595...  \n",
      "3  {'A': 0.21121151745319366, 'B': 0.126120164990...  \n",
      "4  {'A': 0.2177262306213379, 'B': 0.2391255050897...  \n"
     ]
    }
   ],
   "source": [
    "# create pandas dataframe to store the results\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "df = pd.DataFrame(columns=['prompt', 'correct_answer', 'answer_with_max_prob', 'prob_of_correct_answer', 'probabilities'])\n",
    "\n",
    "# iterate over the training set and get the prompt, the correct answer and the probabilities\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "i = 0\n",
    "for item in tqdm.tqdm(raw_dataset['train']):\n",
    "    # i += 1\n",
    "    # if i < 7663:\n",
    "    #     continue\n",
    "    # if i == 7663:\n",
    "    #     print(\"starting from 7660\")\n",
    "    prompt, correct_answer = create_prompt_and_return_correct_answer(item, base_prompt)\n",
    "    probabilities = get_probabilities(prompt)\n",
    "    answer_with_max_prob = max(probabilities, key=probabilities.get)\n",
    "    prob_of_correct_answer = probabilities[correct_answer]\n",
    "    logging.debug(f\"prompt {prompt}\\nprobabilities {probabilities}\\ncorrect_answer {correct_answer}\\nanswer_with_max_prob {answer_with_max_prob}\\n\\n\")\n",
    "    df = df._append({'prompt': prompt, 'correct_answer': correct_answer, 'answer_with_max_prob': answer_with_max_prob, 'prob_of_correct_answer': prob_of_correct_answer, 'probabilities': probabilities}, ignore_index=True)\n",
    "print(df.head())\n",
    " # save the dataframe\n",
    "df.to_csv('meditron.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
